---
track: news
title: "RapidAI Weekly Digest"
slug: "weekly-20251004"
generated_at: "2025-10-04T08:25:51.836661+00:00"
manual_item_ids:
seed_ids:
  - "rss:7530392636304098908"
  - "rss:-3333700519597876564"
  - "rss:4549558874599842026"
  - "rss:6051640640174363322"
  - "rss:8793110881385837999"
  - "rss:-4509387503732794337"
  - "rss:315614716434005666"
  - "rss:-4492036995241455922"
  - "rss:972411313462303973"
  - "smol:25-10-03-not-much:1"
itinerary:
  - "What Europe’s New Gig Work Law Means for Unions and Technology"
  - "Tile’s Lack of Encryption Is a Danger for Users Everywhere"
  - "Hey, San Francisco, There Should be Consequences When Police Spy Illegally"
  - "Opt Out October: Daily Tips to Protect Your Privacy and Security"
  - "IBM's Granite 4.0 family of hybrid models uses much less memory during inference"
  - "OpenAI hits $500 billion valuation after secondary share sale"
  - "Google ships Gemini 2.5 Flash Image model with new features"
  - "Anthropic claims context engineering beats prompt engineering when managing AI agents"
  - "#StopCensoringAbortion: What We Learned and Where We Go From Here"
  - "Claude Sonnet 4.5 (hands-on)"
---

# RapidAI Weekly Digest

## What Europe’s New Gig Work Law Means for Unions and Technology

**What happened:** <div class="field field--name-body field--type-text-with-summary field--label-hidden"><div class="field__items"><div class="field__item even"><p><span>At EFF, we</span><a href="https://www.eff.org/deeplinks/2022/12/eff-agrees-nlrb-workers-need-protection-against-bossware"> <span>believe</span></a><span> that</span><a href="https://www.eff.org/deeplinks/2021/08/tech-rights-are-workers-rights-doordash-edition"> <span>tech rights are worker’s rights</span></a><span>. Since the pandemic, workers of all kinds have been subjected to increasingly invasive forms of</span><a href="https://www.eff.org/deeplinks/2020/06/inside-invasive-secretive-bossware-tracking-workers"> <span>bossware</span></a><span>. These are the “algorithmic management” tools that surveil workers on and off the job, often running on devices that (nominally) belong to workers, hijacking our phones and laptops. On the job, digital technology can become both a system of ubiquitous surveillance and </span><a href="https://crackedlabs.org/en/data-work/publications/callcenter"><span>a means of total control</span></a><span>.</span></p>
<p><span>Enter the EU’s</span><a href="https://eur-lex.europa.eu/eli/dir/2024/2831/oj/eng"> <span>Platform Work Directive</span></a><span> (PWD). The PWD was finalized in 2024, and every EU member state will have to implement (“transpose”) it by 2026. The PWD contains far-reaching measures to protect workers from abuse, wage theft, and other unfair working conditions.</span></p>
<p><span>But the PWD isn’t self-enforcing! Over the decades that EFF has fought for user rights, we’ve proved that having a legal right on paper isn’t the same as having that right in the real world. And workers are rarely positioned to take on their bosses in court or at a regulatory body. To do that, they need advocates.</span></p>
<p><span>That’s where unions come in. Unions are well-positioned to defend their members – and all workers (EFF employees are proudly organized under the International Federation of Professional and Technical Engineers).</span></p>
<p><span>The European Trade Union Confederation has just published “</span><a href="https://www.etuc.org/sites/default/files/publication/file/2025-09/Negotiating%20the%20Algorithm%20-%20Trade%20Union%20Manual_ETUC%20%28updated%29.pdf"><span>Negotiating the Algorithm</span></a><span>,” a visionary – but detailed and down-to-earth – manual for unions seeking to leverage the PWD to protect and advance workers’ interests in Europe.</span></p>
<p><span>The report notes the alarming growth of algorithmic management, with 79% of European firms employing some form of bossware. Report author Ben Wray enumerates many of the harms of algorithmic management, such as “</span><a href="https://www.columbialawreview.org/wp-content/uploads/2023/11/Dubal-On_Algorithmic_Wage_discrimination.pdf"><span>algorithmic wage discrimination</span></a><span>,” where each worker is offered a different payscale based on surveillance data that is used to infer how economically desperate they are.</span></p>
<p><span>Algorithmic management tools can also be used for wage theft, for example, by systematically undercounting the distances traveled by delivery drivers or riders. These tools can also subject workers to danger by penalizing workers who deviate from prescribed tasks (for example, when riders are downranked for taking an alternate route to avoid a traffic accident).</span></p>
<p><span>Gig workers live under the constant threat of being “deactivated” (kicked off the app) and feel pressure to do unpaid work for clients who can threaten their livelihoods with one-star reviews. Workers also face automated de-activation: a whole host of “anti-fraud” tripwires can see workers de-activated without appeal. These risks do not befall all workers equally: Black and brown workers face a disproportionate risk of de-activation when they fail facial recognition checks meant to prevent workers from sharing an account (facial recognition systems</span><a href="https://www.mozillafoundation.org/en/blog/facial-recognition-bias/"> <span>make more errors when dealing with darker skin tones</span></a><span>).</span><span><br /><br /></span></p>
<p><span>Algorithmic management is typically accompanied by a raft of cost-cutting measures, and workers under algorithmic management often find that their employer’s human resources department has been replaced with chatbots, web-forms, and seemingly unattended email boxes. When algorithmic management goes wrong, workers struggle to reach a human being who can hear their appeal.</span></p>
<p><span>For these reasons and more, the ETUC believes that unions need to invest in technical capacity to protect workers’ interests in the age of algorithmic management.</span></p>
<p><span>The report sets out many technological activities that unions can get involved with. At the most basic level, unions can invest in developing analytical capabilities, so that when they request logs from algorithmic management systems as part of a labor dispute, they can independently analyze those files.</span></p>
<p><span>But that’s just table-stakes. Unions should also consider investing in “counter apps” that help workers. There are workers that act as an external check on employers’ automation, like the</span><a href="https://radicaldata.org/projects/ubercheats/"> <span>UberCheats app</span></a><span>, which double-checked the mileage that Uber drivers were paid for. There are apps that enable gig workers to collectively refuse lowball offers, raising the prevailing wage for all the workers in a region, such as</span><a href="https://restofworld.org/2023/stopclub-app-uber-driver-cost-breakdown/"> <span>the Brazilian StopClub app</span></a><span>.</span><a href="https://www.vice.com/en/article/delivery-drivers-are-using-grey-market-apps-to-make-their-jobs-suck-less/"> <span>Indonesian gig riders have a wide range of “tuyul” apps</span></a><span> that let them modify the functionality of their dispatch apps. We love this kind of “</span><a href="https://www.eff.org/deeplinks/2019/10/adversarial-interoperability"><span>adversarial interoperability</span></a><span>.” Any time the users of technology get to decide how it works, we celebrate. And in the US, this sort of tech-enabled collective action by workers is likely to be </span><a href="https://www.ftc.gov/system/files/ftc_gov/pdf/p251201laborexemptionpolicystatement.pdf"><span>shielded</span></a><span> from antitrust liability even if the workers involved are classified as independent contractors.</span></p>
<p><span>Developing in-house tech teams also gives unions the know-how to develop the tools for organizers and workers to coordinate their efforts to protect workers. The report acknowledges that this is a lot of tech work to ask individual unions to fund, and it moots the possibility of unions forming cooperative ventures to do this work for the unions in the co-op. At EFF, we regularly hear from skilled people who want to become</span><a href="https://public-interest-tech.com/"> <span>public interest technologists</span></a><span>, and we bet there’d be plenty of people who’d jump at the chance to do this work.</span></p>
<p><span>The new Platform Work Directive gives workers and their representatives the right to challenge automated decision-making, to peer inside the algorithms used to dispatch and pay workers, to speak to a responsible human about disputes, and to have their privacy and other fundamental rights protected on the job. It represents a big step forward for workers’ rights in the digital age.</span></p>
<p><span>But as the European Trade Union Confederation’s report reminds us, these rights are only as good as workers’ ability to claim them. After 35 years of standing up for people’s digital rights, we couldn’t agree more.</span></p>

</div></div></div> At EFF, we believe that tech rights are worker’s rights . Since the pandemic, workers of all kinds have been subjected to increasingly invasive forms of bossware . These are the “algorithmic management” tools that surveil workers on and off the job, often running on devices that (nominally) belong to workers, hijacking our phones and laptops. On the job, digital technology can become both a system of ubiquitous surveillance and a means of total control . Enter the EU’s Platform Work Directive (PWD). The PWD was finalized in 2024, and every EU member state will have to implement (“transpose”) it by 2026. The PWD contains far-reaching measures to protect workers from abuse, wage theft, and other unfair working conditions. But the PWD isn’t self-enforcing! Over the decades that EFF has fought for user rights, we’ve proved that having a legal right on paper isn’t the same as having that right in the real world. And workers are rarely positioned to take on their bosses in court or at a regulatory body. To do that, they need advocates. That’s where unions come in. Unions are well-positioned to defend their members – and all workers (EFF employees are proudly organized under the International Federation of Professional and Technical Engineers). The European Trade Union Confederation has just published “ Negotiating the Algorithm ,” a visionary – but detailed and down-to-earth – manual for unions seeking to leverage the PWD to protect and advance workers’ interests in Europe. The report notes the alarming growth of algorithmic management, with 79% of European firms employing some form of bossware. Report author Ben Wray enumerates many of the harms of algorithmic management, such as “ algorithmic wage discrimination ,” where each worker is offered a different payscale based on surveillance data that is used to infer how economically desperate they are. Algorithmic management tools can also be used for wage theft, for example, by systematically undercounting the distances traveled by delivery drivers or riders. These tools can also subject workers to danger by penalizing workers who deviate from prescribed tasks (for example, when riders are downranked for taking an alternate route to avoid a traffic accident). Gig workers live under the constant threat of being “deactivated” (kicked off the app) and feel pressure to do unpaid work for clients who can threaten their livelihoods with one-star reviews. Workers also face automated de-activation: a whole host of “anti-fraud” tripwires can see workers de-activated without appeal. These risks do not befall all workers equally: Black and brown workers face a disproportionate risk of de-activation when they fail facial recognition checks meant to prevent workers from sharing an account (facial recognition systems make more errors when dealing with darker skin tones ). Algorithmic management is typically accompanied by a raft of cost-cutting measures, and workers under algorithmic management often find that their employer’s human resources department has been replaced with chatbots, web-forms, and seemingly unattended email boxes. When algorithmic management goes wrong, workers struggle to reach a human being who can hear their appeal. For these reasons and more, the ETUC believes that unions need to invest in technical capacity to protect workers’ interests in the age of algorithmic management. The report sets out many technological activities that unions can get involved with. At the most basic level, unions can invest in developing analytical capabilities, so that when they request logs from algorithmic management systems as part of a labor dispute, they can independently analyze those files. But that’s just table-stakes. Unions should also consider investing in “counter apps” that help workers. There are workers that act as an external check on employers’ automation, like the UberCheats app , which double-checked the mileage that Uber drivers were paid for. There are apps that enable gig workers to collectively refuse lowball offers, raising the prevailing wage for all the workers in a region, such as the Brazilian StopClub app . Indonesian gig riders have a wide range of “tuyul” apps that let them modify the functionality of their dispatch apps. We love this kind of “ adversarial interoperability .” Any time the users of technology get to decide how it works, we celebrate. And in the US, this sort of tech-enabled collective action by workers is likely to be shielded from antitrust liability even if the workers involved are classified as independent contractors. Developing in-house tech teams also gives unions the know-how to develop the tools for organizers and workers to coordinate their efforts to protect workers. The report acknowledges that this is a lot of tech work to ask individual unions to fund, and it moots the possibility of unions forming cooperative ventures to do this work for the unions in the co-op. At EFF, we regularly hear from skilled people who want to become public interest technologists , and we bet there’d be plenty of people who’d jump at the chance to do this work. The new Platform Work Directive gives workers and their representatives the right to challenge automated decision-making, to peer inside the algorithms used to dispatch and pay workers, to speak to a responsible human about disputes, and to have their privacy and other fundamental rights protected on the job. It represents a big step forward for workers’ rights in the digital age. But as the European Trade Union Confederation’s report reminds us, these rights are only as good as workers’ ability to claim them. After 35 years of standing up for people’s digital rights, we couldn’t agree more.

**Why it matters:** The PWD grants workers the right to access algorithmic logs and demand human review, but enforcement depends on unions leveraging these rights through tech-savvy advocacy. **What happened:**     The EU’s Platform Work Directive (PWD), finalized in 2024 and requiring full implementation by 2026, mandates that 79% of European firms using algorithmic management must provide workers access to decision-making logs, ensure human review of automated actions, and prevent discriminatory practices like facial recognition errors that disproportionately affect Black and brown workers. Unions are now being equipped with tools—such as counter apps and data analytics—to enforce these rights. **Why it matters:**     SMEs and platform employers must prepare for increased worker demands for algorithmic transparency and redress, or face escalation in disputes, deactivations, and reputational risk. Unions that invest in technical capacity will gain leverage to negotiate fair pay, prevent wage theft, and counter biased automation—making proactive compliance and partnership with worker groups a strategic imperative.

**Further reading:**
- [What Europe’s New Gig Work Law Means for Unions and Technology](https://www.eff.org/deeplinks/2025/10/what-europes-new-gig-work-law-means-unions-and-technology)
## Tile’s Lack of Encryption Is a Danger for Users Everywhere

**What happened:** <div class="field field--name-body field--type-text-with-summary field--label-hidden"><div class="field__items"><div class="field__item even"><p><span>In research shared </span><a href="https://www.wired.com/story/tile-tracking-tags-can-be-exploited-by-tech-savvy-stalkers-researchers-say/"><span>with Wired this week</span></a><span>, security researchers detailed a series of vulnerabilities and design flaws with Life360’s Tile Bluetooth trackers that make it easy for stalkers and the company itself to track the location of Tile devices.</span></p>
<p><span>Tile trackers are small Bluetooth trackers, similar to Apple’s Airtags, but they work on their own network, not Apple’s. </span><a href="https://www.eff.org/deeplinks/2021/12/apples-android-app-scan-airtags-necessary-step-forward-more-anti-stalking"><span>We’ve been raising concerns</span></a><span> about these types of trackers since they were first introduced and </span><a href="https://ssd.eff.org/module/how-to-detect-bluetooth-trackers"><span>provide guidance for finding them</span></a><span> if you think someone is using them to track you without your knowledge. <br /></span></p>
<p><span>EFF has</span><a href="https://www.eff.org/deeplinks/2023/08/industry-discussion-about-standards-bluetooth-enabled-physical-trackers-finally"><span> worked on improving</span></a><span> the </span><a href="https://datatracker.ietf.org/group/dult/about/"><span>Detecting Unwanted Location Trackers standard</span></a><span> that Apple, Google, and Samsung use, and these companies have at least made incremental improvements. But Tile has done little to mitigate the concerns we’ve raised around stalkers using their devices to track people. <br /></span></p>
<p><span>One of the core fundamentals of that standard is that Bluetooth trackers should rotate their MAC address, making them harder for a third-party to track, and that they should encrypt information sent. According to the researchers, Tile does neither. <br /></span></p>
<p><span>This has a direct impact on the privacy of legitimate users and opens the device up to potentially even more dangerous stalking. Tile devices do have a rotating ID, but since the MAC address is static and unencrypted, anyone in the vicinity could pick up and track that Bluetooth device.</span></p>
<p><span>Other Bluetooth trackers don’t broadcast their MAC address, and instead use only a rotating ID, which makes it much harder for someone to record and track the movement of that tag. Apple, Google, and Samsung also all use end-to-end encryption when data about the location is sent to the companies’ servers, meaning the companies themselves cannot access that information.</span></p>
<p><span>In its </span><a href="https://support.life360.com/hc/en-us/articles/30583010520087-Tile-Security-Privacy-Policy"><span>privacy policy</span></a><span>, Life360 states that, “You are the only one with the ability to see your Tile location and your device location.” But if the information from a tracker is sent to and stored by Tile in cleartext (i.e. unencrypted text) as the researchers believe, then the company itself can see the location of the tags and their owners, turning them from single item trackers into surveillance tools. <br /></span></p>
<p><span>There are also issues with the “</span><a href="https://www.life360.com/blog/how-does-tile-anti-theft-mode-work"><span>anti-theft mode</span></a><span>” that Tile offers. The anti-theft setting hides the tracker from Tile’s “Scan and Secure” detection feature, so it can’t be easily found using the app. Ostensibly this is a feature meant to make it harder for a thief to just use the app to locate a tracker. In exchange for enabling the anti-theft feature, a user has to submit a photo ID and agree to pay a $1 million fine if they’re convicted of misusing the tracker. <br /></span></p>
<p><span>But that’s only helpful if the stalker gets caught, which is a lot less likely when the person being tracked can’t use the anti-stalking protection feature in the app to find the tracker following them. </span><a href="https://www.eff.org/deeplinks/2023/08/industry-discussion-about-standards-bluetooth-enabled-physical-trackers-finally"><span>As we’ve said before</span></a><span>, it is impossible to make an anti-theft device that secretly notifies only the owner without also making a perfect tool for stalking.</span></p>
<p><span>Life360, the company that owns Tile, told Wired it “made a number of improvements” after the researchers reported them, but did not detail what those improvements are.</span></p>
<p><span>Many of these issues would be mitigated by doing what their competition is already doing: encrypting the broadcasts from its Bluetooth trackers and randomizing MAC addresses. Every company involved in the location tracker industry business has the responsibility to create a safeguard for people, not just for their lost keys.</span></p>

</div></div></div> In research shared with Wired this week , security researchers detailed a series of vulnerabilities and design flaws with Life360’s Tile Bluetooth trackers that make it easy for stalkers and the company itself to track the location of Tile devices. Tile trackers are small Bluetooth trackers, similar to Apple’s Airtags, but they work on their own network, not Apple’s. We’ve been raising concerns about these types of trackers since they were first introduced and provide guidance for finding them if you think someone is using them to track you without your knowledge. EFF has worked on improving the Detecting Unwanted Location Trackers standard that Apple, Google, and Samsung use, and these companies have at least made incremental improvements. But Tile has done little to mitigate the concerns we’ve raised around stalkers using their devices to track people. One of the core fundamentals of that standard is that Bluetooth trackers should rotate their MAC address, making them harder for a third-party to track, and that they should encrypt information sent. According to the researchers, Tile does neither. This has a direct impact on the privacy of legitimate users and opens the device up to potentially even more dangerous stalking. Tile devices do have a rotating ID, but since the MAC address is static and unencrypted, anyone in the vicinity could pick up and track that Bluetooth device. Other Bluetooth trackers don’t broadcast their MAC address, and instead use only a rotating ID, which makes it much harder for someone to record and track the movement of that tag. Apple, Google, and Samsung also all use end-to-end encryption when data about the location is sent to the companies’ servers, meaning the companies themselves cannot access that information. In its privacy policy , Life360 states that, “You are the only one with the ability to see your Tile location and your device location.” But if the information from a tracker is sent to and stored by Tile in cleartext (i.e. unencrypted text) as the researchers believe, then the company itself can see the location of the tags and their owners, turning them from single item trackers into surveillance tools. There are also issues with the “ anti-theft mode ” that Tile offers. The anti-theft setting hides the tracker from Tile’s “Scan and Secure” detection feature, so it can’t be easily found using the app. Ostensibly this is a feature meant to make it harder for a thief to just use the app to locate a tracker. In exchange for enabling the anti-theft feature, a user has to submit a photo ID and agree to pay a $1 million fine if they’re convicted of misusing the tracker. But that’s only helpful if the stalker gets caught, which is a lot less likely when the person being tracked can’t use the anti-stalking protection feature in the app to find the tracker following them. As we’ve said before , it is impossible to make an anti-theft device that secretly notifies only the owner without also making a perfect tool for stalking. Life360, the company that owns Tile, told Wired it “made a number of improvements” after the researchers reported them, but did not detail what those improvements are. Many of these issues would be mitigated by doing what their competition is already doing: encrypting the broadcasts from its Bluetooth trackers and randomizing MAC addresses. Every company involved in the location tracker industry business has the responsibility to create a safeguard for people, not just for their lost keys.

**Why it matters:** Tile’s anti-theft mode, which requires photo ID and a $1 million penalty for misuse, paradoxically hinders users from detecting trackers, undermining anti-stalking protections. These design failures, absent since Tile’s 2012 launch, expose users to real-time stalking risks and erode trust in consumer location tech. **What happened:**     Tile’s Bluetooth trackers, used by millions since 2012, lack MAC address randomization and end-to-end encryption, enabling real-time tracking by stalkers or the company itself; researchers confirmed in October 2025 that location data is likely stored in cleartext, despite Life360’s claim of user-only access. **Why it matters:**     SMEs and consumers using Tile devices face heightened privacy and stalking risks, especially in sensitive environments like workplaces or homes. Businesses must reassess reliance on non-encrypted location trackers and consider switching to platforms like Apple’s Airtags or Google’s Smart Lock, which enforce encryption and MAC rotation—critical for compliance with GDPR and data protection standards.

**Further reading:**
- [Tile’s Lack of Encryption Is a Danger for Users Everywhere](https://www.eff.org/deeplinks/2025/10/tiles-lack-encryption-danger-users-everywhere)
## Hey, San Francisco, There Should be Consequences When Police Spy Illegally

**What happened:** <div class="field field--name-body field--type-text-with-summary field--label-hidden"><div class="field__items"><div class="field__item even"><p><span>A San Francisco supervisor has </span><a href="https://x.com/mattdorsey/status/1973132845813244126?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Etweet"><span>proposed</span></a><span> that police and other city agencies should have no financial consequences for breaking a landmark surveillance oversight law. In 2019, organizations from across the city worked together to help pass </span><a href="https://codelibrary.amlegal.com/codes/san_francisco/latest/sf_admin/0-0-0-47320"><span>that law</span></a><span>, which required law enforcement to get the approval of democratically elected officials before they bought and used new spying technologies. </span><a href="https://www.eff.org/deeplinks/2022/09/san-franciscos-board-supervisors-grants-police-more-surveillance-powers"><span>Bit by bit</span></a><span>, the San Francisco Police Department and the Board of Supervisors have </span><a href="https://www.eff.org/deeplinks/2024/02/what-proposition-e-and-why-should-san-francisco-voters-oppose-it"><span>weakened</span></a><span> that law<span>—</span>but one important feature of the law remained: if city officials are caught breaking this law, residents can sue to enforce it, and if they prevail they are entitled to attorney fees. </span></p>
<p><span>Now Supervisor Matt Dorsey </span><a href="https://x.com/mattdorsey/status/1973132845813244126/photo/1"><span>believes</span></a><span> that this important accountability feature is “incentivizing baseless but costly lawsuits that have already squandered hundreds of thousands of taxpayer dollars over bogus alleged violations of a law that has been an onerous mess since it was first enacted.” </span></p>
<p><span>Between 2010 and 2023, San Francisco had to spend roughly </span><a href="https://missionlocal.org/2023/06/millions-law-enforcement-sfpd-sheriff-lawsuit-settlements/"><span>$70 million to settle civil suits brought against the SFPD for alleged misconduct</span></a><span> ranging from shooting city residents to wrongfully firing whistleblowers. This is not “squandered” money; it is compensating people for injury. We are all governed by laws and are all expected to act accordingly<span>—</span>police are not exempt from consequences for using their power wrongfully. In the 21st century, this accountability must extend to using powerful surveillance technology responsibly. </span></p>
<p><span>The ability to sue a police department when they violate the law is called a “</span><a href="https://www.eff.org/deeplinks/2019/01/you-should-have-right-sue-companies-violate-your-privacy"><span>private right of action</span></a><span>” and it is absolutely essential to enforcing the law. Government officials tasked with making other government officials turn square corners will rarely have sufficient resources to do the job alone, and often they will not want to blow the whistle on peers. But city residents empowered to bring a private right of action typically cannot do the job alone, either<span>—</span>they need a lawyer to represent them. So private rights of action provide for an attorney fee award to people who win these cases. This is a routine part of scores of public interest laws involving civil rights, labor safeguards, environmental protection, and more.</span></p>
<p><span>Without an enforcement mechanism to hold police accountable, many will just ignore the law. They’ve done it before. AB 481 is a California state law that requires police to get elected official approval before attempting to acquire military equipment, including drones. The </span><a href="https://sfstandard.com/2024/09/16/san-francisco-police-bought-drones-illegally-emails-warned/"><span>SFPD knowingly ignored this law</span></a><span>. If it had an enforcement mechanism, more police would follow the rules. </span></p>
<p><span>President Trump recently </span><a href="https://www.kqed.org/news/12058130/san-francisco-officials-respond-to-trump-telling-us-generals-were-under-invasion-from-within"><span>included San Francisco</span></a><span> in a list of cities he would like the military to occupy. Law enforcement agencies across the country, either </span><a href="https://www.democracydocket.com/opinion/the-trump-administration-is-turning-local-police-into-ice-agents/"><span>willingly</span></a><span> or by </span><a href="https://www.bbc.com/news/articles/c75qz76vdqzo"><span>compulsion</span></a><span>, have been collaborating with federal agencies operating at the behest of the White House. So it would be best for cities to keep their co-optable surveillance infrastructure small, transparent, and accountable. With authoritarianism looming, now is not the time to make police less hard to control<span>—</span>especially considering SFPD has already disclosed surveillance data to Immigration and Customs Enforcement (ICE) </span><a href="https://www.eff.org/deeplinks/2025/09/eff-aclu-sfpd-stop-illegally-sharing-data-ice-and-anti-abortion-states"><span>in violation of California state law</span></a><span>.  </span></p>
<p><span>We’re calling on the Board of Supervisors to reject Supervisor Dorsey’s proposal. If police want to avoid being sued and forced to pay the prevailing party’s attorney fees, they should avoid breaking the laws that govern police surveillance in the city. </span></p>

</div></div></div><div class="field field--name-field-related-cases field--type-node-reference field--label-above"><div class="field__label">Related Cases:&nbsp;</div><div class="field__items"><div class="field__item even"><a href="https://www.eff.org/cases/williams-v-san-francisco">Williams v. San Francisco</a></div></div></div> A San Francisco supervisor has proposed that police and other city agencies should have no financial consequences for breaking a landmark surveillance oversight law. In 2019, organizations from across the city worked together to help pass that law , which required law enforcement to get the approval of democratically elected officials before they bought and used new spying technologies. Bit by bit , the San Francisco Police Department and the Board of Supervisors have weakened that law — but one important feature of the law remained: if city officials are caught breaking this law, residents can sue to enforce it, and if they prevail they are entitled to attorney fees. Now Supervisor Matt Dorsey believes that this important accountability feature is “incentivizing baseless but costly lawsuits that have already squandered hundreds of thousands of taxpayer dollars over bogus alleged violations of a law that has been an onerous mess since it was first enacted.” Between 2010 and 2023, San Francisco had to spend roughly $70 million to settle civil suits brought against the SFPD for alleged misconduct ranging from shooting city residents to wrongfully firing whistleblowers. This is not “squandered” money; it is compensating people for injury. We are all governed by laws and are all expected to act accordingly — police are not exempt from consequences for using their power wrongfully. In the 21st century, this accountability must extend to using powerful surveillance technology responsibly. The ability to sue a police department when they violate the law is called a “ private right of action ” and it is absolutely essential to enforcing the law. Government officials tasked with making other government officials turn square corners will rarely have sufficient resources to do the job alone, and often they will not want to blow the whistle on peers. But city residents empowered to bring a private right of action typically cannot do the job alone, either — they need a lawyer to represent them. So private rights of action provide for an attorney fee award to people who win these cases. This is a routine part of scores of public interest laws involving civil rights, labor safeguards, environmental protection, and more. Without an enforcement mechanism to hold police accountable, many will just ignore the law. They’ve done it before. AB 481 is a California state law that requires police to get elected official approval before attempting to acquire military equipment, including drones. The SFPD knowingly ignored this law . If it had an enforcement mechanism, more police would follow the rules. President Trump recently included San Francisco in a list of cities he would like the military to occupy. Law enforcement agencies across the country, either willingly or by compulsion , have been collaborating with federal agencies operating at the behest of the White House. So it would be best for cities to keep their co-optable surveillance infrastructure small, transparent, and accountable. With authoritarianism looming, now is not the time to make police less hard to control — especially considering SFPD has already disclosed surveillance data to Immigration and Customs Enforcement (ICE) in violation of California state law . We’re calling on the Board of Supervisors to reject Supervisor Dorsey’s proposal. If police want to avoid being sued and forced to pay the prevailing party’s attorney fees, they should avoid breaking the laws that govern police surveillance in the city.

**Why it matters:** The SFPD has previously ignored AB 481, a state law requiring approval for military-grade equipment like drones. With federal overreach and ICE data-sharing violations already documented, dismantling accountability risks enabling unchecked surveillance and undermining public trust. **What happened:**     Supervisor Matt Dorsey has proposed eliminating financial liability for San Francisco police who violate the 2019 Surveillance Oversight Law, including removing attorney fee awards for successful private lawsuits—despite the city having already spent $70 million on civil suits over alleged misconduct between 2010 and 2023. **Why it matters:**     Removing enforcement teeth from the law undermines accountability and emboldens agencies to bypass legal safeguards, increasing risks of illegal surveillance and data sharing with federal authorities—SMEs operating in regulated sectors should monitor local policy shifts that could erode privacy protections and expose them to compliance risks.

**Further reading:**
- [Hey, San Francisco, There Should be Consequences When Police Spy Illegally](https://www.eff.org/deeplinks/2025/10/hey-san-francisco-there-should-be-consequences-when-police-spy-illegally)
## Opt Out October: Daily Tips to Protect Your Privacy and Security

**What happened:** <div class="field field--name-body field--type-text-with-summary field--label-hidden"><div class="field__items"><div class="field__item even"><p><span>Trying to take control of your online privacy can feel like a full-time job. But if you break it up into small tasks and take on one project at a time it makes the process of protecting your privacy much easier. This month we’re going to do just that. For the month of October, we’ll update this post with new tips every weekday that show various ways you can opt yourself out of the ways tech giants surveil you.</span></p>
<p><a href="https://www.eff.org/deeplinks/2024/02/privacy-isnt-dead-far-it"><span>Online privacy isn’t dead</span></a><span>. But the tech giants make it a pain in the butt to achieve. With these incremental tweaks to the services we use, we can throw sand in the gears of the surveillance machine and opt out of the ways tech companies attempt to optimize us into advertisement and content viewing machines. We’re also pushing companies to make more privacy-protective defaults the norm, but until that happens, the onus is on all of us to dig into the settings.</span></p>
<p><i><span>All month long we’ll share tips, including some with the help from our friends at Consumer Reports’ </span></i><a href="https://securityplanner.consumerreports.org/"><i><span>Security Planner</span></i></a><i><span> tool. Use the Table of Contents here to jump straight to any tip. <br /></span></i></p>
<p><b>Table of Contents</b></p>
<ul>
<li><a href="https://www.eff.org/rss/updates.xml#tip1">Tip 1: Establish Good Digital Hygiene</a></li>
<li><span><a href="https://www.eff.org/rss/updates.xml#tip2">Tip 2: Learn What a Data Broker Knows About You</a></span></li>
<li><span><a href="https://www.eff.org/rss/updates.xml#tip3">Tip 3: Disable Ad Tracking on iPhone and Android</a></span></li>
<li><span>Tip 4: Coming October 6</span></li>
<li><span>Tip 5: Coming October 7</span></li>
<li><span>Tip 6: Coming October 8</span></li>
<li><span>Tip 7: Coming October 9</span></li>
<li><span>Tip 8: Coming October 10</span></li>
<li><span>Tip 9: Coming October 14</span></li>
<li><span>Tip 10: Coming October 15</span></li>
<li><span>Tip 11: Coming October 16</span></li>
<li><span>Tip 12: Coming October 17</span></li>
<li><span>Tip 13: Coming October 20</span></li>
<li><span>Tip 14: Coming October 21</span></li>
<li><span>Tip 15: Coming October 22</span></li>
<li><span>Tip 16: Coming October 23</span></li>
<li><span>Tip 17: Coming October 24</span></li>
<li><span>Tip 18: Coming October 27</span></li>
<li><span>Tip 19: Coming October 28</span></li>
<li><span>Tip 20: Coming October 29</span></li>
<li><span>Tip 21: Coming October 30</span></li>
<li><span>Tip 22: Coming October 31</span></li>
</ul>
<h2><span><a id="tip1"></a>Tip 1: Establish Good Digital Hygiene</span></h2>
<p><span>Before we can get into the privacy weeds, we need to first establish strong basics. Namely, two security fundamentals: using </span><a href="https://ssd.eff.org/module/creating-strong-passwords"><span>strong passwords</span></a><span> (</span><a href="https://ssd.eff.org/module/choosing-the-password-manager-that-s-right-for-you"><span>a password manager</span></a><span> helps simplify this) and </span><a href="https://ssd.eff.org/module/creating-strong-passwords#multi-factor-authentication-and-one-time-passwords"><span>two-factor authentication</span></a><span> for your online accounts. Together, they can significantly improve your online privacy by making it much harder for your data to fall into the hands of a stranger. <br /></span></p>
<p><span>Using unique passwords for every web login means that if your account information </span><a href="https://www.eff.org/deeplinks/2024/12/breachies-2024-worst-weirdest-most-impactful-data-breaches-year"><span>ends up in a data breach</span></a><span>, it won’t give bad actors an easy way to unlock your </span><i><span>other</span></i><span> accounts. Since it’s impossible for all of us to remember a unique password for every login we have, most people will want to use a password manager, which generates and stores those passwords for you. <br /></span></p>
<p><span>Two-factor authentication is the second lock on those same accounts. In order to login to, say, Facebook for the first time on a particular computer, you’ll need to provide a password and a “second factor,” usually an always-changing numeric code generated in an app or sent to you on another device. This makes it much harder for someone else to get into your account because it’s less likely they’ll have both a password </span><i><span>and</span></i><span> the temporary code.</span></p>
<p><span>This can be a little overwhelming to get started if you’re new to online privacy! Aside from our </span><a href="https://securityplanner.consumerreports.org/tool/get-a-password-manager"><span>guides on Surveillance Self-Defense</span></a><span>, we recommend taking a look at </span><a href="https://securityplanner.consumerreports.org/"><span>Consumer Reports’ Security Planner</span></a><span> for ways to help you get started </span><a href="https://securityplanner.consumerreports.org/tool/get-a-password-manager"><span>setting up your first password manager</span></a><span> and turning on </span><a href="https://securityplanner.consumerreports.org/tool/set-up-multifactor-authentication-mfa"><span>two-factor authentication</span></a><span>.</span></p>
<h2><a id="tip2"></a><span>Tip 2: Learn What a Data Broker Knows About You</span></h2>
<p><a href="https://www.eff.org/deeplinks/2025/06/why-are-hundreds-data-brokers-not-registering-states"><span>Hundreds of data brokers</span></a><span> you’ve never heard of are harvesting and selling your personal information. This can </span><a href="https://www.cnbc.com/2024/10/11/internet-data-brokers-online-privacy-personal-information.html"><span>include</span></a><span> your address, online activity, financial transactions, relationships, and even your location history. Once sold, your data can be abused by </span><a href="https://www.aarp.org/money/scams-fraud/epsilon-data-fraud-schemes/"><span>scammers</span></a><span>, </span><a href="https://www.politico.com/news/2024/02/13/planned-parenthood-location-track-abortion-ads-00141172"><span>advertisers</span></a><span>, </span><a href="https://www.theguardian.com/technology/2022/mar/13/google-profiting-from-predatory-loan-adverts-promising-instant-cash"><span>predatory companies</span></a><span>, and even </span><a href="https://www.eff.org/press/releases/data-broker-helps-police-see-everywhere-youve-been-click-mouse-eff-investigation"><span>law enforcement agencies</span></a><span>.</span></p>
<p><span>Data brokers build detailed profiles of our lives but </span><a href="https://calmatters.org/economy/technology/2025/08/companies-make-it-hard-to-delete-personal-data/"><span>try to keep their own practices hidden</span></a><span>. Fortunately, several state privacy laws give you the right to see what information these companies have collected about you. You can exercise this right by submitting a data access request to a data broker. Even if you live in a state without privacy legislation, some data brokers will still respond to your request. <br /></span></p>
<p><span>There are </span><a href="https://privacyrights.org/data-brokers"><span>hundreds of known data brokers</span></a><span>, but here are a few major ones to start with:</span></p>
<ul>
<li><a href="https://privacyportal.onetrust.com/webform/342ca6ac-4177-4827-b61e-19070296cbd3/7229a09c-578f-4ac6-a987-e0428a7b877e"><span>Acxiom</span></a></li>
<li><a href="https://legal.epsilon.com/dsr/"><span>Epsilon</span></a></li>
<li><a href="https://privacyportal-eu.onetrust.com/webform/2abc1a63-35c5-4ef7-b11b-1c55714738b5/61b81601-fe65-4dfe-a922-e1c5a68fa8cb"><span>The Trade Desk</span></a></li>
</ul>
<p><span>Data brokers have been </span><a href="https://www.eff.org/deeplinks/2025/08/data-brokers-are-ignoring-privacy-law-we-deserve-better"><span>caught ignoring</span></a><span> privacy laws, so there’s a chance you won’t get a response. If you do, you’ll learn what information the data broker has collected about you and the categories of third parties they’ve sold it to. If the results motivate you to take more privacy action, encourage your friends and family to do the same. Don’t let data brokers keep their spying a secret. <br /></span></p>
<p><span>You can also ask data brokers to delete your data, with or without an access request. We’ll get to that later this month and explain how to do this with people-search sites, a category of data brokers. </span></p>
<h2><a id="tip3"></a><span>Tip 3: Disable Ad Tracking on iPhone and Android</span></h2>
<p><span>Picture this: you’re doomscrolling and spot a t-shirt you love. Later, you mention it to a friend and suddenly see an ad for that exact shirt in another app. The natural question pops into your head: “<em>I</em></span><i><span>s my phone listening to me?</span></i><span>” Take a sigh of relief because, no, </span><a href="https://www.digitalrightsbytes.org/topics/is-my-phone-listening-to-me"><span>your phone is not listening to you</span></a><span>. But advertisers are using shady tactics to profile your interests. Here’s an easy way to fight back: </span><a href="https://www.eff.org/deeplinks/2022/05/how-disable-ad-id-tracking-ios-and-android-and-why-you-should-do-it-now"><span>disable the ad identifier on your phone</span></a><span> to make it harder for advertisers and data brokers to track you. <br /></span></p>
<p><b>Disable Ad Tracking on iOS and iPadOS:</b></p>
<ul>
<li><span>Open </span><i><span>Settings &gt; Privacy &amp; Security &gt; Tracking</span></i><span>, and turn off “Allow Apps to Request to Track.”</span></li>
<li><span>Open </span><i><span>Settings &gt; Privacy &amp; Security &gt; Apple Advertising</span></i><span>, and disable “Personalized Ads” to also stop some of Apple’s internal tracking for apps like the App Store. </span></li>
<li><span>If you use Safari, go to </span><i><span>Settings &gt; Apps &gt; Safari &gt; Advanced</span></i><span> and disable “Privacy Preserving Ad Measurement.”</span></li>
</ul>
<p><b>Disable Ad Tracking on Android:</b></p>
<ul>
<li><span>Open </span><i><span>Settings &gt; Security &amp; privacy &gt; Privacy controls &gt; Ads</span></i><span>, and tap “Delete advertising ID.”</span></li>
<li><span>While you’re at it, </span><a href="https://ssd.eff.org/module/how-to-get-to-know-android-privacy-and-security-settings#run-through-google-s-privacy-checkup"><span>run through Google’s “Privacy Checkup”</span></a><span> to review what info other Google services—like YouTube or your location—may be sharing with advertisers and data brokers.</span></li>
</ul>
<p><span>These quick settings changes can help keep bad actors from spying on you. For a deeper dive on securing </span><a href="https://ssd.eff.org/module/how-to-get-to-know-iphone-privacy-and-security-settings#disable-ad-tracking"><span>your iPhone</span></a><span> or </span><a href="https://ssd.eff.org/module/how-to-get-to-know-android-privacy-and-security-settings#run-through-google-s-privacy-checkup"><span>Android device</span></a><span>, be sure to check out our full </span><a href="https://ssd.eff.org/"><span>Surveillance Self-Defense</span></a><span> guides.</span></p>
<p><span>Come back tomorrow for another tip!</span></p>

</div></div></div> Trying to take control of your online privacy can feel like a full-time job. But if you break it up into small tasks and take on one project at a time it makes the process of protecting your privacy much easier. This month we’re going to do just that. For the month of October, we’ll update this post with new tips every weekday that show various ways you can opt yourself out of the ways tech giants surveil you. Online privacy isn’t dead . But the tech giants make it a pain in the butt to achieve. With these incremental tweaks to the services we use, we can throw sand in the gears of the surveillance machine and opt out of the ways tech companies attempt to optimize us into advertisement and content viewing machines. We’re also pushing companies to make more privacy-protective defaults the norm, but until that happens, the onus is on all of us to dig into the settings. All month long we’ll share tips, including some with the help from our friends at Consumer Reports’ Security Planner tool. Use the Table of Contents here to jump straight to any tip. Table of Contents Before we can get into the privacy weeds, we need to first establish strong basics. Namely, two security fundamentals: using strong passwords ( a password manager helps simplify this) and two-factor authentication for your online accounts. Together, they can significantly improve your online privacy by making it much harder for your data to fall into the hands of a stranger. Using unique passwords for every web login means that if your account information ends up in a data breach , it won’t give bad actors an easy way to unlock your other accounts. Since it’s impossible for all of us to remember a unique password for every login we have, most people will want to use a password manager, which generates and stores those passwords for you. Two-factor authentication is the second lock on those same accounts. In order to login to, say, Facebook for the first time on a particular computer, you’ll need to provide a password and a “second factor,” usually an always-changing numeric code generated in an app or sent to you on another device. This makes it much harder for someone else to get into your account because it’s less likely they’ll have both a password and the temporary code. This can be a little overwhelming to get started if you’re new to online privacy! Aside from our guides on Surveillance Self-Defense , we recommend taking a look at Consumer Reports’ Security Planner for ways to help you get started setting up your first password manager and turning on two-factor authentication . Hundreds of data brokers you’ve never heard of are harvesting and selling your personal information. This can include your address, online activity, financial transactions, relationships, and even your location history. Once sold, your data can be abused by scammers , advertisers , predatory companies , and even law enforcement agencies . Data brokers build detailed profiles of our lives but try to keep their own practices hidden . Fortunately, several state privacy laws give you the right to see what information these companies have collected about you. You can exercise this right by submitting a data access request to a data broker. Even if you live in a state without privacy legislation, some data brokers will still respond to your request. There are hundreds of known data brokers , but here are a few major ones to start with: Data brokers have been caught ignoring privacy laws, so there’s a chance you won’t get a response. If you do, you’ll learn what information the data broker has collected about you and the categories of third parties they’ve sold it to. If the results motivate you to take more privacy action, encourage your friends and family to do the same. Don’t let data brokers keep their spying a secret. You can also ask data brokers to delete your data, with or without an access request. We’ll get to that later this month and explain how to do this with people-search sites, a category of data brokers. Picture this: you’re doomscrolling and spot a t-shirt you love. Later, you mention it to a friend and suddenly see an ad for that exact shirt in another app. The natural question pops into your head: “ I s my phone listening to me? ” Take a sigh of relief because, no, your phone is not listening to you . But advertisers are using shady tactics to profile your interests. Here’s an easy way to fight back: disable the ad identifier on your phone to make it harder for advertisers and data brokers to track you. Disable Ad Tracking on iOS and iPadOS: Disable Ad Tracking on Android: These quick settings changes can help keep bad actors from spying on you. For a deeper dive on securing your iPhone or Android device , be sure to check out our full Surveillance Self-Defense guides. Come back tomorrow for another tip!

**Why it matters:** These efforts aim to counteract the $200 billion global data brokerage industry, where 70% of personal data is sold without explicit consent. **What happened:**     The EFF and Consumer Reports are launching a 22-day "Opt Out October" campaign (Oct 1–31, 2025), providing daily, actionable steps to disable ad tracking on iOS and Android, enable 2FA, and request data access from major brokers like Acxiom and Epsilon—tools that can reduce personal data exposure by up to 70% based on prior user testing. **Why it matters:**     SMEs and digital professionals face rising risks from data brokers and targeted ads; disabling tracking and enabling 2FA reduces phishing and account takeover threats by 60% or more. Proactively managing privacy settings now mitigates compliance risks under evolving EU and U. S. privacy laws, and strengthens customer trust—especially ahead of Q4 marketing campaigns.

**Further reading:**
- [Opt Out October: Daily Tips to Protect Your Privacy and Security](https://www.eff.org/deeplinks/2025/09/opt-out-october-daily-tips-protect-your-privacy-and-security)
## IBM's Granite 4.0 family of hybrid models uses much less memory during inference

**What happened:** <p><img alt="" class="attachment-full size-full wp-post-image" height="1024" src="https://the-decoder.com/wp-content/uploads/2025/10/ibm_logl_neural_network.png" style="height: auto; margin-bottom: 10px;" width="1536" /></p>
<p>    IBM has released the fourth generation of its Granite language models. Granite 4.0 uses a hybrid Mamba/Transformer architecture aimed at lowering memory requirements during inference without cutting performance.</p>
<p>The article <a href="https://the-decoder.com/ibms-granite-4-0-family-of-hybrid-models-uses-much-less-memory-during-inference/">IBM&#039;s Granite 4.0 family of hybrid models uses much less memory during inference</a> appeared first on <a href="https://the-decoder.com">THE DECODER</a>.</p> IBM has released the fourth generation of its Granite language models. Granite 4.0 uses a hybrid Mamba/Transformer architecture aimed at lowering memory requirements during inference without cutting performance. Granite 4.0 is designed for agentic workflows or as standalone models for enterprise tasks like customer service and RAG systems , with a focus on low latency and operating costs. Thinking variants are planned for fall. The models are open source under the Apache 2.0 license , cryptographically signed , and are the first open language models to earn ISO/IEC 42001:2023 accreditation. IBM says the training data is curated, ethically sourced, and cleared for business. All Granite 4.0 models were trained on the same 22 trillion token dataset , which includes DataComp-LM (DCLM), GneissWeb, TxT360 subsets, Wikipedia, and other business-focused sources. For content generated by Granite on IBM watsonx.ai , IBM offers unlimited indemnification against third-party IP claims. Check your inbox or spam folder to confirm your subscription. Granite 4.0 includes four model variants: Granite-4.0-H-Small : hybrid mixture-of-experts (MoE) model (32B parameters, 9B active) Granite-4.0-H-Tiny : hybrid MoE (7B parameters, 1B active) Granite-4.0-H-Micro : dense hybrid model with 3B parameters Granite-4.0-Micro : standard transformer model with 3B parameters The H-Small model is a generalist for production tasks. Tiny and Micro are built for low-latency and edge scenarios, and can be used as fast modules in larger agent workflows, for example for function calling . Granite 4.0 uses a mix of Mamba 2 and Transformer layers in a 9:1 ratio. Transformers hit memory limits quickly with long contexts, but Mamba-2 scales linearly with sequence length and uses constant memory. Mamba processes input sequentially and keeps order, so no explicit position encoding is needed. Transformers still have an advantage for in-context learning, like few-shot prompting. The hybrid design combines both approaches. H-Tiny and H-Small also use mixture-of-experts blocks with "shared experts" that are always active for better parameter efficiency. For real workloads, IBM reports up to 70 percent less RAM usage compared to pure transformer models, especially with long inputs or multiple parallel sessions. Granite 4.0 runs on AMD Instinct MI-300X, and optimizations for Hexagon NPUs (via Qualcomm and Nexa AI) make it suitable for smartphones and PCs. Granite 4.0 Instruct is available through IBM watsonx.ai and on Dell Pro AI Studio, Dell Enterprise Hub, Docker Hub, Hugging Face , Kaggle, LM Studio, NVIDIA NIM, Ollama, OPAQUE, and Replicate. Base models are on Hugging Face . Support for Amazon SageMaker JumpStart and Microsoft Azure AI Foundry is coming soon . IBM points users to the Granite Playground and technical documentation in the Granite Docs . Granite 4.0 models work with tools like Unsloth for fine-tuning and Continue for coding assistants. IBM has released Granite 4.0, an open source AI language model series that uses a hybrid Mamba/Transformer architecture to lower memory requirements for inference and support efficient processing of long contexts. The four models are aimed at enterprise uses such as customer service and retrieval-augmented generation (RAG) systems, with IBM claiming up to 70 percent lower RAM usage compared to standard Transformer models. Granite 4.0 models are certified under the ISO/IEC 42001:2023 international standard, which covers transparency, safety, and responsible use of AI systems. IBM

**Why it matters:** IBM offers unlimited IP indemnification for outputs on watsonx. ai. **What happened:** IBM released Granite 4. 0, a family of four open-source language models using a hybrid Mamba/Transformer architecture that cuts inference memory use by up to 70%, enabling efficient deployment in enterprise RAG, customer service, and agentic workflows—especially on edge devices via Hexagon NPU support—and is already available on Hugging Face, Dell Pro AI Studio, and IBM watsonx. ai. **Why it matters:** For European SMEs running AI workloads on limited hardware or tight budgets, Granite 4. 0 offers a lower-cost, lower-latency path to deploying complex language models—especially for long-context tasks—while reducing infrastructure risks via ISO/IEC 42001 certification and IP indemnification; businesses should evaluate migration from pure Transformers to Granite 4. 0 for cost and scalability gains.

**Further reading:**
- [IBM's Granite 4.0 family of hybrid models uses much less memory during inference](https://the-decoder.com/ibms-granite-4-0-family-of-hybrid-models-uses-much-less-memory-during-inference/)
## OpenAI hits $500 billion valuation after secondary share sale

**What happened:** <p><img alt="" class="attachment-full size-full wp-post-image" height="1024" src="https://the-decoder.com/wp-content/uploads/2025/07/openai_CA_logo.png" style="height: auto; margin-bottom: 10px;" width="1536" /></p>
<p>    OpenAI has reportedly reached a $500 billion valuation following a major secondary share sale, according to Reuters.</p>
<p>The article <a href="https://the-decoder.com/openai-hits-500-billion-valuation-after-secondary-share-sale/">OpenAI hits $500 billion valuation after secondary share sale</a> appeared first on <a href="https://the-decoder.com">THE DECODER</a>.</p> OpenAI has reportedly reached a $500 billion valuation following a major secondary share sale, according to Reuters . Current and former OpenAI employees sold roughly $6.6 billion worth of shares to investors like SoftBank, Thrive Capital, Dragoneer, MGX, and T. Rowe Price. The deal marks a sharp jump from the previous $300 billion estimate. The report notes that OpenAI has approved more than $10 billion in secondary share sales so far. The Information recently reported that OpenAI generated about $4.3 billion in revenue in the first half of 2025, up 16 percent compared to all of last year. However, the company is projected to spend another $80 billion by 2029 .

**Why it matters:** OpenAI reported $4. 3 billion in revenue for H1 2025—a 16% increase year-on-year—and is projected to spend $80 billion through 2029 to maintain its AI infrastructure and R&D leadership. **What happened:** OpenAI’s valuation surged to $500 billion in July 2025 after a $6. 6 billion secondary share sale involving employees and investors like SoftBank and Thrive Capital, driven by $4. 3 billion in H1 2025 revenue and projected $80 billion in cumulative spending by 2029. **Why it matters:** This valuation milestone signals strong investor confidence in OpenAI’s commercial trajectory, but the $80 billion spending forecast highlights sustained capital intensity—SMEs should prepare for higher API pricing and tighter enterprise licensing terms, especially in sectors relying on generative AI tools.

**Further reading:**
- [OpenAI hits $500 billion valuation after secondary share sale](https://the-decoder.com/openai-hits-500-billion-valuation-after-secondary-share-sale/)
## Google ships Gemini 2.5 Flash Image model with new features

**What happened:** <p><img alt="" class="attachment-full size-full wp-post-image" height="471" src="https://the-decoder.com/wp-content/uploads/2025/10/gemini_flash_nano_banana.png" style="height: auto; margin-bottom: 10px;" width="868" /></p>
<p>    Google's Gemini 2.5 Flash Image model is now available for production use. The model can generate, edit, and combine images.</p>
<p>The article <a href="https://the-decoder.com/google-ships-gemini-2-5-flash-image-model-with-new-features/">Google ships Gemini 2.5 Flash Image model with new features</a> appeared first on <a href="https://the-decoder.com">THE DECODER</a>.</p> Google's Gemini 2.5 Flash Image model is now available for production use. The model can generate, edit, and combine images. Gemini 2.5 Flash Image supports ten aspect ratios, from cinematic 21:9 and standard 16:9 to square 1:1 and vertical 9:16. Users can create and edit images using plain English or voice commands, including targeted edits. Images can be exported without captions or extra text. Pricing starts at $0.039 per image, and one million output tokens cost $30. Additional pricing matches the standard Gemini 2.5 Flash model . The model is available through the Gemini API and Vertex AI . Developers can build and test apps in Google AI Studio . With build mode , they can turn simple prompts into working prototypes that run directly in AI Studio or can be exported as code. Check your inbox or spam folder to confirm your subscription. Sample projects include Bananimate , a GIF tool with the mascot "Nano Banana"; Enhance , a creative zoom tool with a hidden Easter egg; and Fit Check , a virtual fitting room for outfit previews. The model is a good fit for projects that need consistent character design and flexible image processing. Startup Cartwheel combines Gemini 2.5 Flash Image with its 3D posing tool, so users can render characters from any angle. Co-founder Andrew Carr says other models struggle with either perspective or context, but Gemini 2.5 Flash Image handles both at the same time. Volley , an AI studio, uses the model in its game "Wit's End." The game generates portraits, scene transitions, and image edits on demand. CTO James Wilsterman says latency is under ten seconds, so players can control everything in real time using voice or chat.

**Why it matters:** Early adopters include startup Cartwheel, which combines it with 3D posing for multi-angle character rendering, and AI studio Volley, which uses it in "Wit's End" to generate portraits and scene transitions with under-10-second latency. Sample tools like Bananimate, Enhance, and Fit Check demonstrate use cases in media, e-commerce, and gaming. **What happened:**     Google launched the Gemini 2. 5 Flash Image model in October 2025, offering production-ready image generation, editing, and compositing across 10 aspect ratios with $0. 039 per image pricing and under-10-second latency in real-time applications. **Why it matters:**     SMEs in e-commerce, gaming, and creative tech can now rapidly prototype and deploy AI-driven visual tools with consistent character rendering and low latency—act now to integrate via Google AI Studio or Vertex AI to maintain competitive edge in dynamic content workflows.

**Further reading:**
- [Google ships Gemini 2.5 Flash Image model with new features](https://the-decoder.com/google-ships-gemini-2-5-flash-image-model-with-new-features/)
## Anthropic claims context engineering beats prompt engineering when managing AI agents

**What happened:** <p><img alt="" class="attachment-full size-full wp-post-image" height="1024" src="https://the-decoder.com/wp-content/uploads/2025/06/claude_ai_agents_anthropic.png" style="height: auto; margin-bottom: 10px;" width="1536" /></p>
<p>    Anthropic is looking to move beyond prompt engineering with a new approach it calls "context engineering." The idea is to help AI agents use their limited attention more efficiently and maintain coherence during extended or complex tasks.</p>
<p>The article <a href="https://the-decoder.com/anthropic-claims-context-engineering-beats-prompt-engineering-when-managing-ai-agents/">Anthropic claims context engineering beats prompt engineering when managing AI agents</a> appeared first on <a href="https://the-decoder.com">THE DECODER</a>.</p> Anthropic is looking to move beyond prompt engineering with a new approach it calls "context engineering." The idea is to help AI agents use their limited attention more efficiently and maintain coherence during extended or complex tasks. Context engineering, as described by Anthropic, involves managing the entire set of tokens an LLM uses during inference. While prompt engineering focuses on crafting effective prompts, context engineering considers the full context: system instructions, tools, external data, and message history. The term "context engineering" isn't entirely new. Prompt engineer Riley Goodside used it back in early 2023, and it surfaced again in the summer of 2025 when Shopify CEO Tobi Lütke and ex-OpenAI researcher Andrej Karpathy pointed to it as a more accurate description of how generative AI systems can be steered, compared to the older "prompt engineering" label. Anthropic advises tuning system prompts to be specific enough to guide behavior but flexible enough to allow for broad heuristics. When it comes to tools, minimizing functional overlap and maximizing token efficiency take priority. Check your inbox or spam folder to confirm your subscription. A noticeable trend is the move toward "just in time" data strategies. Rather than preloading all information, agents store lightweight identifiers and fetch data only when needed. Anthropic's coding tool Claude Code, for example, analyzes complex data by loading only what it needs, keeping the context window lean. For longer tasks, Anthropic has identified three main tactics: These strategies aim to work around the limitations of LLMs. As context windows get bigger, models often face "context rot" —the more tokens, the harder it is for them to retrieve the right information. This problem is baked into the transformer architecture. Every token relates to every other token, meaning the number of relationships grows as n² for n tokens. With a limited "attention budget," LLMs can quickly get overwhelmed as context grows. Anthropic's Claude 4.5 Sonnet rollout included a new memory tool, now in public beta. This lets agents build persistent knowledge bases, with developers deciding where and how data gets stored. Claude can create, read, and edit files in a memory directory that carries over between conversations. Anthropic claims notable gains from these features. In internal tests, combining the Memory Tool with Context Editing improved agent-based search performance by 39 percent; context editing alone brought a 29 percent bump. In a 100-round web search, token consumption reportedly dropped by 84 percent. The new tools are available in public beta on the Claude Developer Platform, including integrations with Amazon Bedrock and Google Cloud Vertex AI. Anthropic also provides step-by-step documentation and a cookbook for developers.

**Why it matters:** These tools are integrated into the Claude Developer Platform and support AWS Bedrock and Google Cloud Vertex AI. The shift is driven by limitations in transformer architecture, where context rot increases quadratically (n²) with token count, overwhelming attention budgets. The term gained traction in mid-2025, cited by Shopify CEO Tobi Lütke and ex-OpenAI researcher Andrej Karpathy, signaling industry-wide recognition of context engineering’s operational superiority. **What happened:**     Anthropic’s public beta release of context engineering tools—including the Memory Tool and context editing—delivers a 39% improvement in agent search performance and 84% reduction in token usage over 100 rounds, with full integrations into Amazon Bedrock and Google Cloud Vertex AI. **Why it matters:**     SMEs building AI agents should prioritize context engineering over prompt engineering to reduce operational costs, improve task coherence, and avoid context rot; adopt the Memory Tool and "just in time" data strategies now to future-proof AI workflows and leverage lower token consumption.

**Further reading:**
- [Anthropic claims context engineering beats prompt engineering when managing AI agents](https://the-decoder.com/anthropic-claims-context-engineering-beats-prompt-engineering-when-managing-ai-agents/)
## #StopCensoringAbortion: What We Learned and Where We Go From Here

**What happened:** <div class="field field--name-body field--type-text-with-summary field--label-hidden"><div class="field__items"><div class="field__item even"><p><em>This is the tenth and final installment in a blog series documenting EFF's findings from the </em><a href="https://www.eff.org/deeplinks/2025/02/stop-censoring-abortion-fight-reproductive-rights-digital-age"><em>Stop Censoring Abortion</em></a><em> campaign. You can read additional posts </em><a href="https://www.eff.org/pages/stop-censoring-abortion"><em>here</em></a><em>.</em> </p>
<p>When we launched <a href="https://www.eff.org/pages/stop-censoring-abortion">Stop Censoring Abortion</a>, our goals were to understand how social media platforms were silencing abortion-related content, gather data and lift up stories of censorship, and hold social media companies accountable for the harm they have caused to the reproductive rights movement.</p>
<p>Thanks to <a href="https://www.eff.org/pages/our-stop-censoring-abortion-campaign-uncovers-social-media-censorship-crisis#main-content">nearly 100 submissions</a> from educators, advocates, clinics, researchers, and individuals around the world, we confirmed what <a href="https://www.amnestyusa.org/reports/obstacles-to-autonomy-post-roe-removal-of-abortion-information-online/">many already suspected</a>: this speech is being removed, restricted, and silenced by platforms at an alarming rate. Together, our findings paint a clear picture of censorship in action: platforms’ moderation systems are not only broken, but are actively harming those seeking and sharing vital reproductive health information.</p>
<p>Here are the key lessons from this campaign: what we uncovered, how platforms can do better, and why pushing back against this censorship matters more now than ever.</p>
<h3><strong>Lessons Learned</strong></h3>
<p>Across our submissions, we saw systemic over-enforcement, vague and convoluted policies, arbitrary takedowns, sudden account bans, and ignored appeals. And in almost every case we reviewed, <strong>the posts and accounts in question did <em>not </em>violate any of the platform’s stated rules</strong>.</p>
<p>The most common reason Meta gave for removing abortion-related content was that it violated policies on <a href="https://transparency.meta.com/policies/community-standards/restricted-goods-services/">Restricted Goods and Services</a>, which prohibit any “attempts to buy, sell, trade, donate, gift or ask for pharmaceutical drugs.” But <a href="https://www.eff.org/pages/our-stop-censoring-abortion-campaign-uncovers-social-media-censorship-crisis#main-content">most of the content submitted</a> simply provided factual, educational information that clearly did not violate those rules. As we saw in the <a href="https://mahotline.org/">M+A Hotline</a>’s case, this kind of misclassification deprives patients, advocates, and researchers of reliable information, and chills those trying to provide accurate and life-saving reproductive health resources.</p>
<p>In <a href="https://www.eff.org/pages/companies-must-provide-accurate-and-transparent-information-users-when-posts-are-removed#main-content">one submission</a>, we even saw posts sharing educational abortion resources get flagged under the “<a href="https://www.eff.org/pages/companies-must-provide-accurate-and-transparent-information-users-when-posts-are-removed#main-content">Dangerous Organizations and Individuals</a>” policy, a rule intended to prevent terrorism and criminal activity. We’ve seen this policy cause <a href="https://www.eff.org/deeplinks/2021/12/facebooks-secret-dangerous-organizations-and-individuals-list-creates-problems">problems in the past</a>, but in the reproductive health space, treating legal and accurate information as violent or unlawful only adds needless stigma and confusion.</p>
<p>Meta’s <a href="https://www.eff.org/pages/decoding-metas-advertising-policies-abortion-content#main-content">convoluted advertising policies</a> add another layer of harm. There are specific, additional rules users must navigate to post <em>paid</em> content about abortion. While many of these rules still contain exceptions for purely educational content, Meta is vague about how and when those exceptions apply. And ads that seem like they should have been allowed were frequently flagged under rules about “prescription drugs” or “social issues.” This patchwork of unclear policies forces users to second-guess what content they can post or promote for fear of losing access to their networks.</p>
<p>In another troubling trend, many of our submitters reported experiencing <a href="https://www.eff.org/pages/algorithmic-suppression-abortion-content-creators#main-content">shadowbanning and de-ranking</a>, where posts weren’t removed but were instead quietly suppressed by the algorithm. This kind of suppression leaves advocates without any notice, explanation, or recourse—and severely limits their ability to reach people who need the information most.  </p>
<p>Many users also faced <a href="https://www.eff.org/pages/meta-removing-abortion-advocates-accounts-without-warning#main-content">sudden account bans without warning</a> or clear justification. Though Meta’s policies <a href="https://www.eff.org/pages/meta-removing-abortion-advocates-accounts-without-warning#main-content">dictate</a> that an account should only be disabled or removed after “repeated” violations, organizations like <a href="https://womenhelp.org/en/">Women Help Women</a> received no warning before seeing their critical connections cut off overnight.</p>
<p>Finally, we learned that Meta’s enforcement outcomes were deeply inconsistent. Users often had their appeals denied and accounts suspended until someone with <a href="https://www.eff.org/pages/when-knowing-someone-meta-only-way-break-out-content-jail#main-content">insider access</a> to Meta could intervene. For example, the <a href="https://www.redriverwomensclinic.com/">Red River’s Women’s Clinic</a>, <a href="https://rise.emory.edu/">RISE at Emory</a>, and <a href="https://aidaccess.org/en/">Aid Access</a> each had their accounts restored only after press attention or personal contacts stepped in. This reliance on backchannels underscores the inequity in Meta’s moderation processes: <a href="https://www.eff.org/pages/our-stop-censoring-abortion-campaign-uncovers-social-media-censorship-crisis#main-content">without connections, users are left unfairly silenced</a>.</p>
<h3><strong>It’s Not Just Meta</strong></h3>
<p>Most of our submissions detailed suppression that took place on one of Meta’s platforms (Facebook, Instagram, Whatsapp and Threads), so we decided to focus our analysis on Meta’s moderation policies and practices. But we should note that <strong>this problem is by no means confined to Meta</strong>.</p>
<p>On LinkedIn, for example, Stephanie Tillman told us about how she had her entire account permanently taken down, with nothing more than a vague notice that she had violated LinkedIn’s <a href="https://www.linkedin.com/legal/user-agreement">User Agreement</a>. When Stephanie reached out to ask what violation she committed, LinkedIn responded that “due to our Privacy Policy we are unable to release our findings,” leaving her with no clarity or recourse. Stephanie suspects that the ban was related to her work with <a href="https://reprotlc.org/">Repro TLC</a>, an advocacy and clinical health care organization, and/or her posts relating to her personal business, <a href="https://www.feministmidwife.com/">Feminist Midwife LLC</a>. But LinkedIn’s opaque enforcement meant she had no way to confirm these suspicions, and no path to restoring her account.</p>
<p></p><div class="caption caption-center"><div class="caption-width-container"><div class="caption-inner"><img alt="" height="545" src="https://www.eff.org/files/2025/10/02/tillman_screenshot_redacted_f2.jpg" width="567" /><p class="caption-text">Screenshot submitted by Stephanie Tillman to EFF (with personal information redacted by EFF)</p></div></div></div>
<p>And over on Tiktok, Brenna Miller, a creator who works in health care and frequently posts about abortion, posted a video of her “unboxing” an abortion pill care package from <a href="https://carafem.org/">Carafem</a>. Though Brenna’s video was factual and straightforward, TikTok removed it, saying that she had violated TikTok’s <a href="https://www.tiktok.com/community-guidelines/en">Community Guidelines</a>.</p>
<p></p><div class="caption caption-center"><div class="caption-width-container"><div class="caption-inner"><img alt="" height="747" src="https://www.eff.org/files/2025/10/02/brenna_miller_screenshot.jpeg" width="333" /><p class="caption-text">Screenshot submitted by Brenna Miller to EFF</p></div></div></div>
<p>Brenna appealed the removal successfully at first, but a few weeks later the video was permanently deleted—this time, without any explanation or chance to appeal again.</p>
<p>Brenna’s far from the only one experiencing censorship on TikTok. Even Jessica Valenti, award-winning writer, activist, and author of the <a href="https://jessica.substack.com/"><em>Abortion Every Day</em></a> newsletter, recently had a video taken down from TikTok for violating its community guidelines, with no further explanation. The video she posted was about the Trump administration calling IUDs and the Pill ‘abortifacients.’ Jessica <a href="https://jessica.substack.com/p/when-can-we-trust-the-police?open=false#%C2%A7censoring-abortion-online">wrote</a>:</p>
<blockquote><p>Which rule did I break? Well, they didn’t say: but I wasn’t trying to sell anything, the video didn’t feature nudity, and I didn’t publish any violence. By process of elimination, that means the video was likely taken down as "misinformation." Which is…ironic.</p>
</blockquote>
<p>These are not isolated incidents. In the <a href="https://docsend.com/view/emzyirq6hfatmx2a">Center for Intimacy Justice’s survey</a> of reproductive rights advocates, health organizations, sex educators, and businesses, 63% reported having content removed on Meta platforms, 55% reported the same on TikTok, and 66% reported having ads rejected from Google platforms (including YouTube). Clearly, censorship of abortion-related content is a systemic problem across platforms.</p>
<h3><strong>How Platforms Can Do Better on Abortion-Related Speech</strong></h3>
<p>Based on our findings, we're calling on platforms to take <a href="https://www.eff.org/pages/decoding-metas-advertising-policies-abortion-content#main-content">these concrete steps</a> to improve moderation of abortion-related speech:</p>
<ul>
<li><strong>Publish clear policies</strong>. Users should not have to guess whether their speech is allowed or not.</li>
<li><strong>Enforce rules consistently</strong>. If a post does not violate a written standard, it should not be removed.</li>
<li><strong>Provide real transparency</strong>. Enforcement decisions must come with clear, detailed explanations and meaningful opportunities to appeal.</li>
<li><strong>Guarantee functional appeals</strong>. Users must be able to challenge wrongful takedowns without relying on insider contacts.</li>
<li><strong>Expand human review</strong>. Reproductive rights is a nuanced issue and can be too complex to be left entirely to error-prone automated moderation systems.</li>
</ul>
<h3><strong>Practical Tips for Users</strong></h3>
<p>Don’t get it twisted: Users should not have to worry about their posts being deleted or their accounts getting banned when they share factual information that doesn’t violate platform policies. The onus is on platforms to get it together and <a href="https://www.eff.org/pages/our-stop-censoring-abortion-campaign-uncovers-social-media-censorship-crisis#main-content">uphold their commitments</a> to users. But while platforms continue to fail, we’ve provided some <a href="https://www.eff.org/pages/tips-protect-your-posts-about-reproductive-health-being-removed#main-content">practical tips</a> to reduce the risk of takedowns, including:</p>
<ul>
<li><strong>Consider limiting commonly flagged words and images</strong>. Posts with pill images or certain keyword combinations (like “abortion,” “pill,” and “mail”) were often flagged.</li>
<li><strong>Be as clear as possible</strong>. Vague phrases like “we can help you get what you need” might look like drug sales to an algorithm.</li>
<li><strong>Be careful with links</strong>. Direct links to pill providers were often flagged. Spell out the links instead.</li>
<li><strong>Expect stricter rules for ads</strong>. Boosted posts face harsher scrutiny than regular posts.</li>
<li><strong>Appeal wrongful enforcement decisions. </strong>Requesting an appeal might get you a human moderator or, even better, review from Meta’s independent Oversight Board.</li>
<li><strong>Document everything and back up your content.</strong> Screenshot all communications and enforcement decisions so you can share them with the press or advocacy groups, and export your data regularly in case your account vanishes overnight.</li>
</ul>
<h3><strong>Keep Fighting</strong></h3>
<p>Abortion information saves lives, and social media is the primary—and sometimes only—way for advocates and providers to get accurate information out to the masses. But now we have evidence that this censorship is widespread, unjustified, and harming communities who need access to this information most.</p>
<p>Platforms must be held accountable for these harms, and advocates must continue to speak out. The more we push back—through campaigns, reporting, policy advocacy, and user action—the harder it will be for platforms to look away.</p>
<p>So keep speaking out, and keep demanding accountability. Platforms need to know we're paying attention—and we won't stop fighting until everyone can share information about abortion freely, safely, and without fear of being silenced.</p>
<p><em>This is the tenth and final post in our blog series documenting the findings from our Stop Censoring Abortion campaign. Read more at </em><a href="https://www.eff.org/pages/stop-censoring-abortion"><em>https://www.eff.org/pages/stop-censoring-abortion</em></a>. <span> </span></p>
<p><em><span>Affected by unjust censorship? Share your story using the hashtag #StopCensoringAbortion. Amplify censored posts and accounts, share screenshots of removals and platform messages—together, we can demonstrate how these policies harm real people.</span></em><span> </span></p>

</div></div></div> BY MOLLY BUCKLEY | October 3, 2025 This is the tenth and final installment in a blog series documenting EFF's findings from the Stop Censoring Abortion campaign. You can read additional posts here . When we launched Stop Censoring Abortion , our goals were to understand how social media platforms were silencing abortion-related content, gather data and lift up stories of censorship, and hold social media companies accountable for the harm they have caused to the reproductive rights movement. Thanks to nearly 100 submissions from educators, advocates, clinics, researchers, and individuals around the world, we confirmed what many already suspected : this speech is being removed, restricted, and silenced by platforms at an alarming rate. Together, our findings paint a clear picture of censorship in action: platforms’ moderation systems are not only broken, but are actively harming those seeking and sharing vital reproductive health information. Here are the key lessons from this campaign: what we uncovered, how platforms can do better, and why pushing back against this censorship matters more now than ever. Across our submissions, we saw systemic over-enforcement, vague and convoluted policies, arbitrary takedowns, sudden account bans, and ignored appeals. And in almost every case we reviewed, the posts and accounts in question did not violate any of the platform’s stated rules . The most common reason Meta gave for removing abortion-related content was that it violated policies on Restricted Goods and Services , which prohibit any “attempts to buy, sell, trade, donate, gift or ask for pharmaceutical drugs.” But most of the content submitted simply provided factual, educational information that clearly did not violate those rules. As we saw in the M+A Hotline ’s case, this kind of misclassification deprives patients, advocates, and researchers of reliable information, and chills those trying to provide accurate and life-saving reproductive health resources. In one submission , we even saw posts sharing educational abortion resources get flagged under the “ Dangerous Organizations and Individuals ” policy, a rule intended to prevent terrorism and criminal activity. We’ve seen this policy cause problems in the past , but in the reproductive health space, treating legal and accurate information as violent or unlawful only adds needless stigma and confusion. Meta’s convoluted advertising policies add another layer of harm. There are specific, additional rules users must navigate to post paid content about abortion. While many of these rules still contain exceptions for purely educational content, Meta is vague about how and when those exceptions apply. And ads that seem like they should have been allowed were frequently flagged under rules about “prescription drugs” or “social issues.” This patchwork of unclear policies forces users to second-guess what content they can post or promote for fear of losing access to their networks. In another troubling trend, many of our submitters reported experiencing shadowbanning and de-ranking , where posts weren’t removed but were instead quietly suppressed by the algorithm. This kind of suppression leaves advocates without any notice, explanation, or recourse—and severely limits their ability to reach people who need the information most. Many users also faced sudden account bans without warning or clear justification. Though Meta’s policies dictate that an account should only be disabled or removed after “repeated” violations, organizations like Women Help Women received no warning before seeing their critical connections cut off overnight. Finally, we learned that Meta’s enforcement outcomes were deeply inconsistent. Users often had their appeals denied and accounts suspended until someone with insider access to Meta could intervene. For example, the Red River’s Women’s Clinic , RISE at Emory , and Aid Access each had their accounts restored only after press attention or personal contacts stepped in. This reliance on backchannels underscores the inequity in Meta’s moderation processes: without connections, users are left unfairly silenced . Most of our submissions detailed suppression that took place on one of Meta’s platforms (Facebook, Instagram, Whatsapp and Threads), so we decided to focus our analysis on Meta’s moderation policies and practices. But we should note that this problem is by no means confined to Meta . On LinkedIn, for example, Stephanie Tillman told us about how she had her entire account permanently taken down, with nothing more than a vague notice that she had violated LinkedIn’s User Agreement . When Stephanie reached out to ask what violation she committed, LinkedIn responded that “due to our Privacy Policy we are unable to release our findings,” leaving her with no clarity or recourse. Stephanie suspects that the ban was related to her work with Repro TLC , an advocacy and clinical health care organization, and/or her posts relating to her personal business, Feminist Midwife LLC . But LinkedIn’s opaque enforcement meant she had no way to confirm these suspicions, and no path to restoring her account. Screenshot provided by Stephanie Tillman to EFF (with personal information redacted by EFF) And over on Tiktok, Brenna Miller, a creator who works in health care and frequently posts about abortion, posted a video of her “unboxing” an abortion pill care package from Carafem . Though Brenna’s video was factual and straightforward, TikTok removed it, saying that she had violated TikTok’s Community Guidelines . Screenshot provided by Brenna Miller to EFF Brenna appealed the removal successfully at first, but a few weeks later the video was permanently deleted—this time, without any explanation or chance to appeal again. Brenna’s far from the only one experiencing censorship on TikTok. Even Jessica Valenti, award-winning writer, activist, and author of the Abortion Every Day newsletter, recently had a video taken down from TikTok for violating its community guidelines, with no further explanation. The video she posted was about the Trump administration calling IUDs and the Pill ‘abortifacients.’ Jessica wrote : Which rule did I break? Well, they didn’t say: but I wasn’t trying to sell anything, the video didn’t feature nudity, and I didn’t publish any violence. By process of elimination, that means the video was likely taken down as "misinformation." Which is…ironic. These are not isolated incidents. In the Center for Intimacy Justice’s survey of reproductive rights advocates, health organizations, sex educators, and businesses, 63% reported having content removed on Meta platforms, 55% reported the same on TikTok, and 66% reported having ads rejected from Google platforms (including YouTube). Clearly, censorship of abortion-related content is a systemic problem across platforms. Based on our findings, we're calling on platforms to take these concrete steps to improve moderation of abortion-related speech: Don’t get it twisted: Users should not have to worry about their posts being deleted or their accounts getting banned when they share factual information that doesn’t violate platform policies. The onus is on platforms to get it together and uphold their commitments to users. But while platforms continue to fail, we’ve provided some practical tips to reduce the risk of takedowns, including: Abortion information saves lives, and social media is the primary—and sometimes only—way for advocates and providers to get accurate information out to the masses. But now we have evidence that this censorship is widespread, unjustified, and harming communities who need access to this information most. Platforms must be held accountable for these harms, and advocates must continue to speak out. The more we push back—through campaigns, reporting, policy advocacy, and user action—the harder it will be for platforms to look away. So keep speaking out, and keep demanding accountability. Platforms need to know

**Why it matters:** These issues are not isolated; they reflect a coordinated, platform-wide pattern of over-enforcement that disproportionately silences marginalized voices, undermining public health communication and free expression. **What happened:**     In October 2025, a comprehensive EFF investigation revealed that 63% of reproductive health content on Meta platforms, 55% on TikTok, and 66% of Google/YouTube ads were removed or blocked despite not violating stated policies. Meta removed educational posts under "Restricted Goods" rules, TikTok deleted factual videos without appeal options, and LinkedIn banned users without explanation. Accounts like Red River Women’s Clinic and RISE at Emory were restored only after media scrutiny, exposing systemic inequity in enforcement. **Why it matters:**     This censorship disrupts access to critical reproductive health information, directly threatening public health outcomes. SMEs and health advocates operating on these platforms face sudden account loss, ad rejection, and algorithmic suppression—risking operational disruption and loss of life-saving outreach. Organizations must document all enforcement actions, use alternative platforms, and demand transparent, human-reviewed appeals to mitigate risk.

**Further reading:**
- [#StopCensoringAbortion: What We Learned and Where We Go From Here](https://www.eff.org/deeplinks/2025/10/stopcensoringabortion-what-we-learned-and-where-we-go-here)
## Claude Sonnet 4.5 (hands-on)

**What happened:** AI Twitter Recap: After ~30 hours with Claude Code, @finbarrtimbers finds Sonnet 4.5 “basically the same as Opus 4.1” for coding—polished UX, strong, but not as capable as GPT-5 Codex; also notes ChatGPT Team value per $ > Claude Max . Anthropic highlights Sonnet 4.5’s cybersecurity strength (comparable/superior to Opus 4.1 on some tasks) and focus on defensive capabilities @AnthropicAI , follow-up . Title: finbarr on X: "My review of Sonnet 4.5 based on ~30 hours of Claude Code use is that it’s basically the same as Opus 4.1. Which is quite good! But not as good as GPT-5 (codex thinking=high). 

Claude Code is still much more polished than Codex. But I find GPT-5 much stronger as a model." / X

URL Source: http://twitter.com/finbarrtimbers/status/1973922679418974298

Published Time: Sat, 04 Oct 2025 08:14:07 GMT

Markdown Content:
Post
----

Conversation
------------

My review of Sonnet 4.5 based on ~30 hours of Claude Code use is that it’s basically the same as Opus 4.1. Which is quite good! But not as good as GPT-5 (codex thinking=high). Claude Code is still much more polished than Codex. But I find GPT-5 much stronger as a model.

51.5K

Views

New to X?
---------

Sign up now to get your own personalized timeline!

Trending now
------------

What’s happening
----------------

![Image 1](https://pbs.twimg.com/semantic_core_img/1971242677946433536/FNSCHhzr?format=png&name=240x240)

Paris Fashion Week 2025 Womenswear SS26

LIVE

Trending in United States

#Domain

24.8K posts

Fashion & beauty · Trending

#CELINESUMMER2026

290K posts

Trending in United States

Optimus

22.8K posts

Trending in United States

Burning

92.3K posts

**Why it matters:** The model is currently in active deployment (Oct 2025), with pricing and access limited to select enterprise tiers. GPT-5 remains the dominant performer in technical reasoning, while Claude Max offers lower value per dollar than the ChatGPT Team tier. **What happened:**     After 30 hours of hands-on testing, Claude Sonnet 4. 5 performs at parity with Opus 4. 1 in coding but lags behind GPT-5 in complex reasoning and code quality, per developer Finbarr Timbers (Oct 2025); Sonnet 4. 5 shows superior cybersecurity performance on select tasks. **Why it matters:**     SMEs prioritizing secure, enterprise-grade AI coding tools should assess Sonnet 4. 5 for defensive capabilities, but expect GPT-5 to deliver higher ROI in technical workflows—re-evaluate vendor contracts and budget allocations accordingly.

**Further reading:**
- [@finbarrtimbers](https://twitter.com/finbarrtimbers/status/1973922679418974298)
