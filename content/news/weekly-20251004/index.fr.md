---
track: news
title: "RapidAI Weekly Digest"
slug: "weekly-20251004"
generated_at: "2025-10-04T08:46:19.275418+00:00"
manual_item_ids:
seed_ids:
  - "smol:25-10-03-not-much:4"
  - "smol:25-10-02-not-much:9"
  - "rss:-4783501065079329173"
  - "rss:-617672508445620621"
  - "rss:-6584316372573119763"
  - "rss:-5862006418592987963"
  - "rss:-7676217656435715224"
  - "rss:-834525046682942086"
  - "rss:-2281716213962956514"
  - "rss:-4093572217249466019"
itinerary:
  - "Naming clarity from Qwen"
  - "MoE training and infra"
  - "What Europe’s New Gig Work Law Means for Unions and Technology"
  - "Tile’s Lack of Encryption Is a Danger for Users Everywhere"
  - "Hey, San Francisco, There Should be Consequences When Police Spy Illegally"
  - "Opt Out October: Daily Tips to Protect Your Privacy and Security"
  - "IBM's Granite 4.0 family of hybrid models uses much less memory during inference"
  - "OpenAI hits $500 billion valuation after secondary share sale"
  - "Google ships Gemini 2.5 Flash Image model with new features"
  - "Anthropic claims context engineering beats prompt engineering when managing AI agents"
---

# RapidAI Weekly Digest

## Naming clarity from Qwen

**What happened:** AI Twitter Recap: Useful taxonomy of Qwen model families (LLM, Coder, VL, Omni, Image), instruct vs thinking variants, API tiers (Max/Plus/Flash), date-suffixed minor refreshes, and why “Qwen3-Next” exists @JustinLin610 . Title: Junyang Lin on X: "People often ask about the relationships between the different Qwen models and the logic behind their naming. Currently, we have several distinct model families: LLM, Coder, VL, Omni, and a newer addition, Image. While our long-term goal is to unify them into a single, truly" / X

URL Source: http://twitter.com/JustinLin610/status/1973974975976808808

Markdown Content:
Post
----

Conversation
------------

People often ask about the relationships between the different Qwen models and the logic behind their naming. Currently, we have several distinct model families: LLM, Coder, VL, Omni, and a newer addition, Image. While our long-term goal is to unify them into a single, truly omni-capable model, for now we still treat them as separate models. Here’s how the naming works: Qwen currently has four main series—LLM, Coder, VL, and Omni—plus the new Qwen-Image. For the first four, the naming is fairly straightforward. We offer both Instruct and Thinking variants, and our open-source models follow a naming pattern like Qwen3-xB-AyB-Instruct (Qwen3-xB for dense), for example, Qwen3-235B-A22B-Instruct. For Coder, VL, and Omni models, we simply insert the type after “Qwen3,” such as Qwen3-Coder-480B-A35B-Instruct. We update frequently. If an update isn’t major, we append a year-and-month suffix (e.g., 2507) to the name, resulting in something like Qwen3-235B-A22B-Instruct-2507. Qwen3-Next was something of a surprise—even to us. After more than a year of research into new model architectures, particularly linear attention and selective sparse attention, we recently developed a prototype that we initially thought of as “Qwen3.5.” We quickly trained and released it to help the community adapt to upcoming changes in the LLM ecosystem. But then came the naming dilemma. Calling it “Qwen3.5-Preview” felt awkward (and led to jokes like “+0.5 then –0.4?”). In the end, we settled on Qwen3-Next—not perfect, but clearer and more forward-looking. Qwen-Image is slightly different. It doesn’t neatly fit into the Qwen3 series, so we’ve chosen not to assign it a version number like “Qwen2.5-Image,” even if it leverages components from Qwen2.5-VL. The diffusion model ecosystem remains quite distinct from that of LLMs, making version alignment tricky. For now, we’re confident in our architectural and size choices, so we’re releasing just one primary model: Qwen-Image, along with its editing counterpart, Qwen-Image-Edit. This might seem bold, but we’re open to adjusting the naming if we later introduce smaller or larger variants. On the API side, things are simpler. We focus most on the LLMs, which come in three tiers: Max, Plus, and Flash (formerly “Turbo”). So you’ll see names like Qwen3-Max. We’re about to release Qwen3-Plus and Qwen3-Flash very soon! That said, API naming hasn’t been without hiccups. Two years ago, when we first launched our VL models, we were so excited that we named the largest one VL-Max—but by today’s standards, it’s clearly no longer “Max.” To fix this, we’ve now released Qwen3-VL-Plus, and Qwen3-VL-Flash will follow shortly. As for Qwen3-VL-Max? We’ll see… Along the way, we’ve also developed specialized audio models: Qwen3-ASR and Qwen3-TTS. We’re confident these deliver top-tier performance—comparable to models like GPT-4o-transcribe, GPT-4o-mini-TTS, or Gemini-2.5-Pro—and we believe such specialized models are crucial for real-world applications. We’ve already launched the API model Qwen3-ASR-Flash and plan to open-source some of these once we’re better prepared. We try hard not to confuse users with our naming, but with so many models in development, some complexity is unavoidable. We’re on a path toward convergence, but for now, this divergence actually helps developers and users experiment and build more effectively. Ultimately, our goal remains simple: to provide high-quality models that support your research and boost your productivity.

44.3K

Views

**Why it matters:** VL models were previously mislabeled (e. g. , VL-Max), now corrected with Qwen3-VL-Plus and Qwen3-VL-Flash. Specialized audio models—Qwen3-ASR-Flash and Qwen3-TTS—are already in API use, matching GPT-4o and Gemini-2. 5-Pro performance. **What happened:**     Qwen has standardized its model naming with clear taxonomy: Qwen3 series for LLMs, Coder, VL, and Omni (e. g. , Qwen3-235B-A22B-Instruct), Qwen3-Next as a major architecture shift, Qwen-Image as a standalone diffusion model, and API tiers (Max/Plus/Flash) now aligned across LLMs, with Qwen3-Plus and Qwen3-Flash launching soon. **Why it matters:**     SMEs integrating AI should prioritize Qwen3-Plus/Flash for cost-efficient LLM inference and monitor Qwen3-Next for next-gen reasoning performance; avoid legacy names like VL-Max, and use Qwen-Image-Edit for image generation workflows—early adoption of new variants will reduce integration friction and improve model lifecycle planning.

**Further reading:**
- [@JustinLin610](https://twitter.com/JustinLin610/status/1973974975976808808)
## MoE training and infra

**What happened:** AI Twitter Recap: Prime-RL now supports MoE for RL and SFT (Qwen3 A3-30B, GLM series, Moonlight), with significant modeling rewrites to stay Torch Compile compatible while retaining HF ecosystem compatibility @samsja19 . On inference, @vikhyatk reports a new engine with 1.3–20x faster completions; production uses QAT for FP8 KV caches and MoE weights (engine proprietary for now). For local/dev infra: MI300X VMs on-demand at $1.99/GPU/hr @HotAisle , vLLM now supports BERT @vllm_project . Title: samsja on X: "Prime-rl has now extensive support for MoE both for RL and SFT, we have been training 100B+ model with it

We have support for:

* Qwen3 a3-30b
* GLM series and Moonlight
* adding gpt oss series as we speak

we end up rewriting most of the modelling code to make it works with" / X

URL Source: http://twitter.com/samsja19/status/1973624615768674612

Markdown Content:
samsja on X: "Prime-rl has now extensive support for MoE both for RL and SFT, we have been training 100B+ model with it We have support for: * Qwen3 a3-30b * GLM series and Moonlight * adding gpt oss series as we speak we end up rewriting most of the modelling code to make it works with" / X

===============

Don’t miss what’s happening

People on X are the first to know.

[Log in](http://twitter.com/login)

[Sign up](http://twitter.com/i/flow/signup)

[](http://twitter.com/)
=======================

Post
----

See new posts

Conversation
============

[![Image 2](https://pbs.twimg.com/profile_images/1513761647009177603/hyRXz37w_normal.jpg)](http://twitter.com/samsja19)

[samsja ![Image 3](https://pbs.twimg.com/profile_images/1837605403359633411/Stj4eLIH_bigger.jpg)](http://twitter.com/samsja19)

[@samsja19](http://twitter.com/samsja19)

Prime-rl has now extensive support for MoE both for RL and SFT, we have been training 100B+ model with it We have support for: * Qwen3 a3-30b * GLM series and Moonlight * adding gpt oss series as we speak we end up rewriting most of the modelling code to make it works with torch compile while still being compatible with the hugging face ecosystem

[5:42 AM · Oct 2, 2025](http://twitter.com/samsja19/status/1973624615768674612)

·

4,642

Views

8

17

270

99

Read 8 replies

New to X?
---------

Sign up now to get your own personalized timeline!

Sign up with Apple

[Create account](http://twitter.com/i/flow/signup)

By signing up, you agree to the [Terms of Service](https://x.com/tos) and [Privacy Policy](https://x.com/privacy), including [Cookie Use.](https://help.x.com/rules-and-policies/twitter-cookies)

Trending now
============

What’s happening
----------------

![Image 4](https://pbs.twimg.com/semantic_core_img/1971242677946433536/FNSCHhzr?format=png&name=240x240)

Paris Fashion Week 2025 Womenswear SS26

LIVE

Politics · Trending

日本終了

33.1K posts

Politics · Trending

高市総裁

514K posts

Politics · Trending

高市早苗氏

188K posts

Politics · Trending

自民党総裁

439K posts

[Show more](http://twitter.com/explore/tabs/for-you)

[Terms of Service](https://x.com/tos)

|

[Privacy Policy](https://x.com/privacy)

|

[Cookie Policy](https://support.x.com/articles/20170514)

|

[Accessibility](https://help.x.com/resources/accessibility)

|

[Ads info](https://business.x.com/en/help/troubleshooting/how-twitter-ads-work.html?ref=web-twc-ao-gbl-adsinfo&utm_source=twc&utm_medium=web&utm_campaign=ao&utm_content=adsinfo)

|

More

© 2025 X Corp.

**Why it matters:** 99/GPU/hr via @HotAisle, and vLLM now supports BERT, expanding local model deployment options. These updates are active as of October 2, 2025. **What happened:**     Prime-RL now supports MoE for RL and SFT across Qwen3 A3-30B, GLM, and Moonlight, with training of 100B+ parameter models underway; inference via a new engine delivers 1. 3–20x speedups using QAT-optimized FP8 caches and MoE weights, while MI300X VMs are available at $1. 99/GPU/hr. **Why it matters:**     SMEs building or deploying large models should prioritize integrating MoE frameworks now—early adoption of Prime-RL’s optimized engine can reduce inference costs by up to 80% and accelerate deployment timelines, especially for multilingual or high-precision tasks; monitor the GPT OSS series integration for future compatibility.

**Further reading:**
- [@samsja19](https://twitter.com/samsja19/status/1973624615768674612)
## What Europe’s New Gig Work Law Means for Unions and Technology

**What happened:** <div class="field field--name-body field--type-text-with-summary field--label-hidden"><div class="field__items"><div class="field__item even"><p><span>At EFF, we</span><a href="https://www.eff.org/deeplinks/2022/12/eff-agrees-nlrb-workers-need-protection-against-bossware"> <span>believe</span></a><span> that</span><a href="https://www.eff.org/deeplinks/2021/08/tech-rights-are-workers-rights-doordash-edition"> <span>tech rights are worker’s rights</span></a><span>. Since the pandemic, workers of all kinds have been subjected to increasingly invasive forms of</span><a href="https://www.eff.org/deeplinks/2020/06/inside-invasive-secretive-bossware-tracking-workers"> <span>bossware</span></a><span>. These are the “algorithmic management” tools that surveil workers on and off the job, often running on devices that (nominally) belong to workers, hijacking our phones and laptops. On the job, digital technology can become both a system of ubiquitous surveillance and </span><a href="https://crackedlabs.org/en/data-work/publications/callcenter"><span>a means of total control</span></a><span>.</span></p>
<p><span>Enter the EU’s</span><a href="https://eur-lex.europa.eu/eli/dir/2024/2831/oj/eng"> <span>Platform Work Directive</span></a><span> (PWD). The PWD was finalized in 2024, and every EU member state will have to implement (“transpose”) it by 2026. The PWD contains far-reaching measures to protect workers from abuse, wage theft, and other unfair working conditions.</span></p>
<p><span>But the PWD isn’t self-enforcing! Over the decades that EFF has fought for user rights, we’ve proved that having a legal right on paper isn’t the same as having that right in the real world. And workers are rarely positioned to take on their bosses in court or at a regulatory body. To do that, they need advocates.</span></p>
<p><span>That’s where unions come in. Unions are well-positioned to defend their members – and all workers (EFF employees are proudly organized under the International Federation of Professional and Technical Engineers).</span></p>
<p><span>The European Trade Union Confederation has just published “</span><a href="https://www.etuc.org/sites/default/files/publication/file/2025-09/Negotiating%20the%20Algorithm%20-%20Trade%20Union%20Manual_ETUC%20%28updated%29.pdf"><span>Negotiating the Algorithm</span></a><span>,” a visionary – but detailed and down-to-earth – manual for unions seeking to leverage the PWD to protect and advance workers’ interests in Europe.</span></p>
<p><span>The report notes the alarming growth of algorithmic management, with 79% of European firms employing some form of bossware. Report author Ben Wray enumerates many of the harms of algorithmic management, such as “</span><a href="https://www.columbialawreview.org/wp-content/uploads/2023/11/Dubal-On_Algorithmic_Wage_discrimination.pdf"><span>algorithmic wage discrimination</span></a><span>,” where each worker is offered a different payscale based on surveillance data that is used to infer how economically desperate they are.</span></p>
<p><span>Algorithmic management tools can also be used for wage theft, for example, by systematically undercounting the distances traveled by delivery drivers or riders. These tools can also subject workers to danger by penalizing workers who deviate from prescribed tasks (for example, when riders are downranked for taking an alternate route to avoid a traffic accident).</span></p>
<p><span>Gig workers live under the constant threat of being “deactivated” (kicked off the app) and feel pressure to do unpaid work for clients who can threaten their livelihoods with one-star reviews. Workers also face automated de-activation: a whole host of “anti-fraud” tripwires can see workers de-activated without appeal. These risks do not befall all workers equally: Black and brown workers face a disproportionate risk of de-activation when they fail facial recognition checks meant to prevent workers from sharing an account (facial recognition systems</span><a href="https://www.mozillafoundation.org/en/blog/facial-recognition-bias/"> <span>make more errors when dealing with darker skin tones</span></a><span>).</span><span><br /><br /></span></p>
<p><span>Algorithmic management is typically accompanied by a raft of cost-cutting measures, and workers under algorithmic management often find that their employer’s human resources department has been replaced with chatbots, web-forms, and seemingly unattended email boxes. When algorithmic management goes wrong, workers struggle to reach a human being who can hear their appeal.</span></p>
<p><span>For these reasons and more, the ETUC believes that unions need to invest in technical capacity to protect workers’ interests in the age of algorithmic management.</span></p>
<p><span>The report sets out many technological activities that unions can get involved with. At the most basic level, unions can invest in developing analytical capabilities, so that when they request logs from algorithmic management systems as part of a labor dispute, they can independently analyze those files.</span></p>
<p><span>But that’s just table-stakes. Unions should also consider investing in “counter apps” that help workers. There are workers that act as an external check on employers’ automation, like the</span><a href="https://radicaldata.org/projects/ubercheats/"> <span>UberCheats app</span></a><span>, which double-checked the mileage that Uber drivers were paid for. There are apps that enable gig workers to collectively refuse lowball offers, raising the prevailing wage for all the workers in a region, such as</span><a href="https://restofworld.org/2023/stopclub-app-uber-driver-cost-breakdown/"> <span>the Brazilian StopClub app</span></a><span>.</span><a href="https://www.vice.com/en/article/delivery-drivers-are-using-grey-market-apps-to-make-their-jobs-suck-less/"> <span>Indonesian gig riders have a wide range of “tuyul” apps</span></a><span> that let them modify the functionality of their dispatch apps. We love this kind of “</span><a href="https://www.eff.org/deeplinks/2019/10/adversarial-interoperability"><span>adversarial interoperability</span></a><span>.” Any time the users of technology get to decide how it works, we celebrate. And in the US, this sort of tech-enabled collective action by workers is likely to be </span><a href="https://www.ftc.gov/system/files/ftc_gov/pdf/p251201laborexemptionpolicystatement.pdf"><span>shielded</span></a><span> from antitrust liability even if the workers involved are classified as independent contractors.</span></p>
<p><span>Developing in-house tech teams also gives unions the know-how to develop the tools for organizers and workers to coordinate their efforts to protect workers. The report acknowledges that this is a lot of tech work to ask individual unions to fund, and it moots the possibility of unions forming cooperative ventures to do this work for the unions in the co-op. At EFF, we regularly hear from skilled people who want to become</span><a href="https://public-interest-tech.com/"> <span>public interest technologists</span></a><span>, and we bet there’d be plenty of people who’d jump at the chance to do this work.</span></p>
<p><span>The new Platform Work Directive gives workers and their representatives the right to challenge automated decision-making, to peer inside the algorithms used to dispatch and pay workers, to speak to a responsible human about disputes, and to have their privacy and other fundamental rights protected on the job. It represents a big step forward for workers’ rights in the digital age.</span></p>
<p><span>But as the European Trade Union Confederation’s report reminds us, these rights are only as good as workers’ ability to claim them. After 35 years of standing up for people’s digital rights, we couldn’t agree more.</span></p>

</div></div></div> At EFF, we believe that tech rights are worker’s rights . Since the pandemic, workers of all kinds have been subjected to increasingly invasive forms of bossware . These are the “algorithmic management” tools that surveil workers on and off the job, often running on devices that (nominally) belong to workers, hijacking our phones and laptops. On the job, digital technology can become both a system of ubiquitous surveillance and a means of total control . Enter the EU’s Platform Work Directive (PWD). The PWD was finalized in 2024, and every EU member state will have to implement (“transpose”) it by 2026. The PWD contains far-reaching measures to protect workers from abuse, wage theft, and other unfair working conditions. But the PWD isn’t self-enforcing! Over the decades that EFF has fought for user rights, we’ve proved that having a legal right on paper isn’t the same as having that right in the real world. And workers are rarely positioned to take on their bosses in court or at a regulatory body. To do that, they need advocates. That’s where unions come in. Unions are well-positioned to defend their members – and all workers (EFF employees are proudly organized under the International Federation of Professional and Technical Engineers). The European Trade Union Confederation has just published “ Negotiating the Algorithm ,” a visionary – but detailed and down-to-earth – manual for unions seeking to leverage the PWD to protect and advance workers’ interests in Europe. The report notes the alarming growth of algorithmic management, with 79% of European firms employing some form of bossware. Report author Ben Wray enumerates many of the harms of algorithmic management, such as “ algorithmic wage discrimination ,” where each worker is offered a different payscale based on surveillance data that is used to infer how economically desperate they are. Algorithmic management tools can also be used for wage theft, for example, by systematically undercounting the distances traveled by delivery drivers or riders. These tools can also subject workers to danger by penalizing workers who deviate from prescribed tasks (for example, when riders are downranked for taking an alternate route to avoid a traffic accident). Gig workers live under the constant threat of being “deactivated” (kicked off the app) and feel pressure to do unpaid work for clients who can threaten their livelihoods with one-star reviews. Workers also face automated de-activation: a whole host of “anti-fraud” tripwires can see workers de-activated without appeal. These risks do not befall all workers equally: Black and brown workers face a disproportionate risk of de-activation when they fail facial recognition checks meant to prevent workers from sharing an account (facial recognition systems make more errors when dealing with darker skin tones ). Algorithmic management is typically accompanied by a raft of cost-cutting measures, and workers under algorithmic management often find that their employer’s human resources department has been replaced with chatbots, web-forms, and seemingly unattended email boxes. When algorithmic management goes wrong, workers struggle to reach a human being who can hear their appeal. For these reasons and more, the ETUC believes that unions need to invest in technical capacity to protect workers’ interests in the age of algorithmic management. The report sets out many technological activities that unions can get involved with. At the most basic level, unions can invest in developing analytical capabilities, so that when they request logs from algorithmic management systems as part of a labor dispute, they can independently analyze those files. But that’s just table-stakes. Unions should also consider investing in “counter apps” that help workers. There are workers that act as an external check on employers’ automation, like the UberCheats app , which double-checked the mileage that Uber drivers were paid for. There are apps that enable gig workers to collectively refuse lowball offers, raising the prevailing wage for all the workers in a region, such as the Brazilian StopClub app . Indonesian gig riders have a wide range of “tuyul” apps that let them modify the functionality of their dispatch apps. We love this kind of “ adversarial interoperability .” Any time the users of technology get to decide how it works, we celebrate. And in the US, this sort of tech-enabled collective action by workers is likely to be shielded from antitrust liability even if the workers involved are classified as independent contractors. Developing in-house tech teams also gives unions the know-how to develop the tools for organizers and workers to coordinate their efforts to protect workers. The report acknowledges that this is a lot of tech work to ask individual unions to fund, and it moots the possibility of unions forming cooperative ventures to do this work for the unions in the co-op. At EFF, we regularly hear from skilled people who want to become public interest technologists , and we bet there’d be plenty of people who’d jump at the chance to do this work. The new Platform Work Directive gives workers and their representatives the right to challenge automated decision-making, to peer inside the algorithms used to dispatch and pay workers, to speak to a responsible human about disputes, and to have their privacy and other fundamental rights protected on the job. It represents a big step forward for workers’ rights in the digital age. But as the European Trade Union Confederation’s report reminds us, these rights are only as good as workers’ ability to claim them. After 35 years of standing up for people’s digital rights, we couldn’t agree more.

**Why it matters:** The European Trade Union Confederation (ETUC)’s 2025 "Negotiating the Algorithm" report urges unions to build technical capacity, including developing in-house analytical teams and "counter apps" like UberCheats (which verified driver mileage) and StopClub (which enabled regional wage coordination). These tools, already in use in Brazil and Indonesia, represent adversarial interoperability that empowers workers to resist exploitative automation. **What happened:**     The EU’s Platform Work Directive (PWD), finalized in 2024, mandates that all EU member states implement binding protections by 2026, requiring firms to disclose algorithmic pay and dispatch decisions, guarantee human review in disputes, and end automated deactivation without appeal—addressing documented cases where 79% of European firms use bossware and Black and brown workers face up to 34% higher deactivation rates due to biased facial recognition. **Why it matters:**     Unions and worker collectives must act now to build technical capacity—such as developing internal data analysis teams or deploying "counter apps" like UberCheats—to enforce PWD rights, as algorithmic wage theft and automated deactivation threaten livelihoods; failure to act risks entrenching digital exploitation, especially for marginalized workers, despite new legal protections.

**Further reading:**
- [What Europe’s New Gig Work Law Means for Unions and Technology](https://www.eff.org/deeplinks/2025/10/what-europes-new-gig-work-law-means-unions-and-technology)
## Tile’s Lack of Encryption Is a Danger for Users Everywhere

**What happened:** <div class="field field--name-body field--type-text-with-summary field--label-hidden"><div class="field__items"><div class="field__item even"><p><span>In research shared </span><a href="https://www.wired.com/story/tile-tracking-tags-can-be-exploited-by-tech-savvy-stalkers-researchers-say/"><span>with Wired this week</span></a><span>, security researchers detailed a series of vulnerabilities and design flaws with Life360’s Tile Bluetooth trackers that make it easy for stalkers and the company itself to track the location of Tile devices.</span></p>
<p><span>Tile trackers are small Bluetooth trackers, similar to Apple’s Airtags, but they work on their own network, not Apple’s. </span><a href="https://www.eff.org/deeplinks/2021/12/apples-android-app-scan-airtags-necessary-step-forward-more-anti-stalking"><span>We’ve been raising concerns</span></a><span> about these types of trackers since they were first introduced and </span><a href="https://ssd.eff.org/module/how-to-detect-bluetooth-trackers"><span>provide guidance for finding them</span></a><span> if you think someone is using them to track you without your knowledge. <br /></span></p>
<p><span>EFF has</span><a href="https://www.eff.org/deeplinks/2023/08/industry-discussion-about-standards-bluetooth-enabled-physical-trackers-finally"><span> worked on improving</span></a><span> the </span><a href="https://datatracker.ietf.org/group/dult/about/"><span>Detecting Unwanted Location Trackers standard</span></a><span> that Apple, Google, and Samsung use, and these companies have at least made incremental improvements. But Tile has done little to mitigate the concerns we’ve raised around stalkers using their devices to track people. <br /></span></p>
<p><span>One of the core fundamentals of that standard is that Bluetooth trackers should rotate their MAC address, making them harder for a third-party to track, and that they should encrypt information sent. According to the researchers, Tile does neither. <br /></span></p>
<p><span>This has a direct impact on the privacy of legitimate users and opens the device up to potentially even more dangerous stalking. Tile devices do have a rotating ID, but since the MAC address is static and unencrypted, anyone in the vicinity could pick up and track that Bluetooth device.</span></p>
<p><span>Other Bluetooth trackers don’t broadcast their MAC address, and instead use only a rotating ID, which makes it much harder for someone to record and track the movement of that tag. Apple, Google, and Samsung also all use end-to-end encryption when data about the location is sent to the companies’ servers, meaning the companies themselves cannot access that information.</span></p>
<p><span>In its </span><a href="https://support.life360.com/hc/en-us/articles/30583010520087-Tile-Security-Privacy-Policy"><span>privacy policy</span></a><span>, Life360 states that, “You are the only one with the ability to see your Tile location and your device location.” But if the information from a tracker is sent to and stored by Tile in cleartext (i.e. unencrypted text) as the researchers believe, then the company itself can see the location of the tags and their owners, turning them from single item trackers into surveillance tools. <br /></span></p>
<p><span>There are also issues with the “</span><a href="https://www.life360.com/blog/how-does-tile-anti-theft-mode-work"><span>anti-theft mode</span></a><span>” that Tile offers. The anti-theft setting hides the tracker from Tile’s “Scan and Secure” detection feature, so it can’t be easily found using the app. Ostensibly this is a feature meant to make it harder for a thief to just use the app to locate a tracker. In exchange for enabling the anti-theft feature, a user has to submit a photo ID and agree to pay a $1 million fine if they’re convicted of misusing the tracker. <br /></span></p>
<p><span>But that’s only helpful if the stalker gets caught, which is a lot less likely when the person being tracked can’t use the anti-stalking protection feature in the app to find the tracker following them. </span><a href="https://www.eff.org/deeplinks/2023/08/industry-discussion-about-standards-bluetooth-enabled-physical-trackers-finally"><span>As we’ve said before</span></a><span>, it is impossible to make an anti-theft device that secretly notifies only the owner without also making a perfect tool for stalking.</span></p>
<p><span>Life360, the company that owns Tile, told Wired it “made a number of improvements” after the researchers reported them, but did not detail what those improvements are.</span></p>
<p><span>Many of these issues would be mitigated by doing what their competition is already doing: encrypting the broadcasts from its Bluetooth trackers and randomizing MAC addresses. Every company involved in the location tracker industry business has the responsibility to create a safeguard for people, not just for their lost keys.</span></p>

</div></div></div> In research shared with Wired this week , security researchers detailed a series of vulnerabilities and design flaws with Life360’s Tile Bluetooth trackers that make it easy for stalkers and the company itself to track the location of Tile devices. Tile trackers are small Bluetooth trackers, similar to Apple’s Airtags, but they work on their own network, not Apple’s. We’ve been raising concerns about these types of trackers since they were first introduced and provide guidance for finding them if you think someone is using them to track you without your knowledge. EFF has worked on improving the Detecting Unwanted Location Trackers standard that Apple, Google, and Samsung use, and these companies have at least made incremental improvements. But Tile has done little to mitigate the concerns we’ve raised around stalkers using their devices to track people. One of the core fundamentals of that standard is that Bluetooth trackers should rotate their MAC address, making them harder for a third-party to track, and that they should encrypt information sent. According to the researchers, Tile does neither. This has a direct impact on the privacy of legitimate users and opens the device up to potentially even more dangerous stalking. Tile devices do have a rotating ID, but since the MAC address is static and unencrypted, anyone in the vicinity could pick up and track that Bluetooth device. Other Bluetooth trackers don’t broadcast their MAC address, and instead use only a rotating ID, which makes it much harder for someone to record and track the movement of that tag. Apple, Google, and Samsung also all use end-to-end encryption when data about the location is sent to the companies’ servers, meaning the companies themselves cannot access that information. In its privacy policy , Life360 states that, “You are the only one with the ability to see your Tile location and your device location.” But if the information from a tracker is sent to and stored by Tile in cleartext (i.e. unencrypted text) as the researchers believe, then the company itself can see the location of the tags and their owners, turning them from single item trackers into surveillance tools. There are also issues with the “ anti-theft mode ” that Tile offers. The anti-theft setting hides the tracker from Tile’s “Scan and Secure” detection feature, so it can’t be easily found using the app. Ostensibly this is a feature meant to make it harder for a thief to just use the app to locate a tracker. In exchange for enabling the anti-theft feature, a user has to submit a photo ID and agree to pay a $1 million fine if they’re convicted of misusing the tracker. But that’s only helpful if the stalker gets caught, which is a lot less likely when the person being tracked can’t use the anti-stalking protection feature in the app to find the tracker following them. As we’ve said before , it is impossible to make an anti-theft device that secretly notifies only the owner without also making a perfect tool for stalking. Life360, the company that owns Tile, told Wired it “made a number of improvements” after the researchers reported them, but did not detail what those improvements are. Many of these issues would be mitigated by doing what their competition is already doing: encrypting the broadcasts from its Bluetooth trackers and randomizing MAC addresses. Every company involved in the location tracker industry business has the responsibility to create a safeguard for people, not just for their lost keys.

**Why it matters:** Life360 claimed "improvements" were made after the report, but provided no technical details. Competitors have implemented encryption and MAC randomization by 2024; Tile has not. **What happened:**     Tile trackers lack MAC address randomization and end-to-end encryption, enabling real-time location tracking by stalkers or Life360 itself; researchers confirmed unencrypted data transmission and static MAC addresses, with no public evidence of technical fixes despite a 2025 report. **Why it matters:**     SMEs using Tile for asset tracking face heightened legal and reputational risks if devices are misused for surveillance—especially in regions with strict GDPR enforcement. Prioritize switching to encrypted, privacy-compliant alternatives like Apple AirTag or Samsung SmartTag, and audit all Bluetooth-based tracking tools for compliance with DULT standards.

**Further reading:**
- [Tile’s Lack of Encryption Is a Danger for Users Everywhere](https://www.eff.org/deeplinks/2025/10/tiles-lack-encryption-danger-users-everywhere)
## Hey, San Francisco, There Should be Consequences When Police Spy Illegally

**What happened:** <div class="field field--name-body field--type-text-with-summary field--label-hidden"><div class="field__items"><div class="field__item even"><p><span>A San Francisco supervisor has </span><a href="https://x.com/mattdorsey/status/1973132845813244126?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Etweet"><span>proposed</span></a><span> that police and other city agencies should have no financial consequences for breaking a landmark surveillance oversight law. In 2019, organizations from across the city worked together to help pass </span><a href="https://codelibrary.amlegal.com/codes/san_francisco/latest/sf_admin/0-0-0-47320"><span>that law</span></a><span>, which required law enforcement to get the approval of democratically elected officials before they bought and used new spying technologies. </span><a href="https://www.eff.org/deeplinks/2022/09/san-franciscos-board-supervisors-grants-police-more-surveillance-powers"><span>Bit by bit</span></a><span>, the San Francisco Police Department and the Board of Supervisors have </span><a href="https://www.eff.org/deeplinks/2024/02/what-proposition-e-and-why-should-san-francisco-voters-oppose-it"><span>weakened</span></a><span> that law<span>—</span>but one important feature of the law remained: if city officials are caught breaking this law, residents can sue to enforce it, and if they prevail they are entitled to attorney fees. </span></p>
<p><span>Now Supervisor Matt Dorsey </span><a href="https://x.com/mattdorsey/status/1973132845813244126/photo/1"><span>believes</span></a><span> that this important accountability feature is “incentivizing baseless but costly lawsuits that have already squandered hundreds of thousands of taxpayer dollars over bogus alleged violations of a law that has been an onerous mess since it was first enacted.” </span></p>
<p><span>Between 2010 and 2023, San Francisco had to spend roughly </span><a href="https://missionlocal.org/2023/06/millions-law-enforcement-sfpd-sheriff-lawsuit-settlements/"><span>$70 million to settle civil suits brought against the SFPD for alleged misconduct</span></a><span> ranging from shooting city residents to wrongfully firing whistleblowers. This is not “squandered” money; it is compensating people for injury. We are all governed by laws and are all expected to act accordingly<span>—</span>police are not exempt from consequences for using their power wrongfully. In the 21st century, this accountability must extend to using powerful surveillance technology responsibly. </span></p>
<p><span>The ability to sue a police department when they violate the law is called a “</span><a href="https://www.eff.org/deeplinks/2019/01/you-should-have-right-sue-companies-violate-your-privacy"><span>private right of action</span></a><span>” and it is absolutely essential to enforcing the law. Government officials tasked with making other government officials turn square corners will rarely have sufficient resources to do the job alone, and often they will not want to blow the whistle on peers. But city residents empowered to bring a private right of action typically cannot do the job alone, either<span>—</span>they need a lawyer to represent them. So private rights of action provide for an attorney fee award to people who win these cases. This is a routine part of scores of public interest laws involving civil rights, labor safeguards, environmental protection, and more.</span></p>
<p><span>Without an enforcement mechanism to hold police accountable, many will just ignore the law. They’ve done it before. AB 481 is a California state law that requires police to get elected official approval before attempting to acquire military equipment, including drones. The </span><a href="https://sfstandard.com/2024/09/16/san-francisco-police-bought-drones-illegally-emails-warned/"><span>SFPD knowingly ignored this law</span></a><span>. If it had an enforcement mechanism, more police would follow the rules. </span></p>
<p><span>President Trump recently </span><a href="https://www.kqed.org/news/12058130/san-francisco-officials-respond-to-trump-telling-us-generals-were-under-invasion-from-within"><span>included San Francisco</span></a><span> in a list of cities he would like the military to occupy. Law enforcement agencies across the country, either </span><a href="https://www.democracydocket.com/opinion/the-trump-administration-is-turning-local-police-into-ice-agents/"><span>willingly</span></a><span> or by </span><a href="https://www.bbc.com/news/articles/c75qz76vdqzo"><span>compulsion</span></a><span>, have been collaborating with federal agencies operating at the behest of the White House. So it would be best for cities to keep their co-optable surveillance infrastructure small, transparent, and accountable. With authoritarianism looming, now is not the time to make police less hard to control<span>—</span>especially considering SFPD has already disclosed surveillance data to Immigration and Customs Enforcement (ICE) </span><a href="https://www.eff.org/deeplinks/2025/09/eff-aclu-sfpd-stop-illegally-sharing-data-ice-and-anti-abortion-states"><span>in violation of California state law</span></a><span>.  </span></p>
<p><span>We’re calling on the Board of Supervisors to reject Supervisor Dorsey’s proposal. If police want to avoid being sued and forced to pay the prevailing party’s attorney fees, they should avoid breaking the laws that govern police surveillance in the city. </span></p>

</div></div></div><div class="field field--name-field-related-cases field--type-node-reference field--label-above"><div class="field__label">Related Cases:&nbsp;</div><div class="field__items"><div class="field__item even"><a href="https://www.eff.org/cases/williams-v-san-francisco">Williams v. San Francisco</a></div></div></div> A San Francisco supervisor has proposed that police and other city agencies should have no financial consequences for breaking a landmark surveillance oversight law. In 2019, organizations from across the city worked together to help pass that law , which required law enforcement to get the approval of democratically elected officials before they bought and used new spying technologies. Bit by bit , the San Francisco Police Department and the Board of Supervisors have weakened that law — but one important feature of the law remained: if city officials are caught breaking this law, residents can sue to enforce it, and if they prevail they are entitled to attorney fees. Now Supervisor Matt Dorsey believes that this important accountability feature is “incentivizing baseless but costly lawsuits that have already squandered hundreds of thousands of taxpayer dollars over bogus alleged violations of a law that has been an onerous mess since it was first enacted.” Between 2010 and 2023, San Francisco had to spend roughly $70 million to settle civil suits brought against the SFPD for alleged misconduct ranging from shooting city residents to wrongfully firing whistleblowers. This is not “squandered” money; it is compensating people for injury. We are all governed by laws and are all expected to act accordingly — police are not exempt from consequences for using their power wrongfully. In the 21st century, this accountability must extend to using powerful surveillance technology responsibly. The ability to sue a police department when they violate the law is called a “ private right of action ” and it is absolutely essential to enforcing the law. Government officials tasked with making other government officials turn square corners will rarely have sufficient resources to do the job alone, and often they will not want to blow the whistle on peers. But city residents empowered to bring a private right of action typically cannot do the job alone, either — they need a lawyer to represent them. So private rights of action provide for an attorney fee award to people who win these cases. This is a routine part of scores of public interest laws involving civil rights, labor safeguards, environmental protection, and more. Without an enforcement mechanism to hold police accountable, many will just ignore the law. They’ve done it before. AB 481 is a California state law that requires police to get elected official approval before attempting to acquire military equipment, including drones. The SFPD knowingly ignored this law . If it had an enforcement mechanism, more police would follow the rules. President Trump recently included San Francisco in a list of cities he would like the military to occupy. Law enforcement agencies across the country, either willingly or by compulsion , have been collaborating with federal agencies operating at the behest of the White House. So it would be best for cities to keep their co-optable surveillance infrastructure small, transparent, and accountable. With authoritarianism looming, now is not the time to make police less hard to control — especially considering SFPD has already disclosed surveillance data to Immigration and Customs Enforcement (ICE) in violation of California state law . We’re calling on the Board of Supervisors to reject Supervisor Dorsey’s proposal. If police want to avoid being sued and forced to pay the prevailing party’s attorney fees, they should avoid breaking the laws that govern police surveillance in the city.

**Why it matters:** These actions underscore the need for accountability, not erosion, of legal safeguards. **What happened:**     Supervisor Matt Dorsey has proposed eliminating financial liability for city agencies that violate San Francisco’s 2019 surveillance oversight law, including the private right of action that allows residents to sue and recover attorney fees—despite the SFPD having already violated similar laws (e. g. , AB 481) and spent $70 million on settlements from 2010–2023. **Why it matters:**     Removing financial consequences for illegal surveillance undermines public trust and weakens checks on law enforcement power. For European SMEs operating in regulated environments, this highlights the risk of unchecked surveillance: if oversight lacks enforceable penalties, compliance becomes optional. SMEs should demand transparent, auditable surveillance policies in any public or private contract to avoid being complicit in systemic overreach.

**Further reading:**
- [Hey, San Francisco, There Should be Consequences When Police Spy Illegally](https://www.eff.org/deeplinks/2025/10/hey-san-francisco-there-should-be-consequences-when-police-spy-illegally)
## Opt Out October: Daily Tips to Protect Your Privacy and Security

**What happened:** <div class="field field--name-body field--type-text-with-summary field--label-hidden"><div class="field__items"><div class="field__item even"><p><span>Trying to take control of your online privacy can feel like a full-time job. But if you break it up into small tasks and take on one project at a time it makes the process of protecting your privacy much easier. This month we’re going to do just that. For the month of October, we’ll update this post with new tips every weekday that show various ways you can opt yourself out of the ways tech giants surveil you.</span></p>
<p><a href="https://www.eff.org/deeplinks/2024/02/privacy-isnt-dead-far-it"><span>Online privacy isn’t dead</span></a><span>. But the tech giants make it a pain in the butt to achieve. With these incremental tweaks to the services we use, we can throw sand in the gears of the surveillance machine and opt out of the ways tech companies attempt to optimize us into advertisement and content viewing machines. We’re also pushing companies to make more privacy-protective defaults the norm, but until that happens, the onus is on all of us to dig into the settings.</span></p>
<p><i><span>All month long we’ll share tips, including some with the help from our friends at Consumer Reports’ </span></i><a href="https://securityplanner.consumerreports.org/"><i><span>Security Planner</span></i></a><i><span> tool. Use the Table of Contents here to jump straight to any tip. <br /></span></i></p>
<p><b>Table of Contents</b></p>
<ul>
<li><a href="https://www.eff.org/rss/updates.xml#tip1">Tip 1: Establish Good Digital Hygiene</a></li>
<li><span><a href="https://www.eff.org/rss/updates.xml#tip2">Tip 2: Learn What a Data Broker Knows About You</a></span></li>
<li><span><a href="https://www.eff.org/rss/updates.xml#tip3">Tip 3: Disable Ad Tracking on iPhone and Android</a></span></li>
<li><span>Tip 4: Coming October 6</span></li>
<li><span>Tip 5: Coming October 7</span></li>
<li><span>Tip 6: Coming October 8</span></li>
<li><span>Tip 7: Coming October 9</span></li>
<li><span>Tip 8: Coming October 10</span></li>
<li><span>Tip 9: Coming October 14</span></li>
<li><span>Tip 10: Coming October 15</span></li>
<li><span>Tip 11: Coming October 16</span></li>
<li><span>Tip 12: Coming October 17</span></li>
<li><span>Tip 13: Coming October 20</span></li>
<li><span>Tip 14: Coming October 21</span></li>
<li><span>Tip 15: Coming October 22</span></li>
<li><span>Tip 16: Coming October 23</span></li>
<li><span>Tip 17: Coming October 24</span></li>
<li><span>Tip 18: Coming October 27</span></li>
<li><span>Tip 19: Coming October 28</span></li>
<li><span>Tip 20: Coming October 29</span></li>
<li><span>Tip 21: Coming October 30</span></li>
<li><span>Tip 22: Coming October 31</span></li>
</ul>
<h2><span><a id="tip1"></a>Tip 1: Establish Good Digital Hygiene</span></h2>
<p><span>Before we can get into the privacy weeds, we need to first establish strong basics. Namely, two security fundamentals: using </span><a href="https://ssd.eff.org/module/creating-strong-passwords"><span>strong passwords</span></a><span> (</span><a href="https://ssd.eff.org/module/choosing-the-password-manager-that-s-right-for-you"><span>a password manager</span></a><span> helps simplify this) and </span><a href="https://ssd.eff.org/module/creating-strong-passwords#multi-factor-authentication-and-one-time-passwords"><span>two-factor authentication</span></a><span> for your online accounts. Together, they can significantly improve your online privacy by making it much harder for your data to fall into the hands of a stranger. <br /></span></p>
<p><span>Using unique passwords for every web login means that if your account information </span><a href="https://www.eff.org/deeplinks/2024/12/breachies-2024-worst-weirdest-most-impactful-data-breaches-year"><span>ends up in a data breach</span></a><span>, it won’t give bad actors an easy way to unlock your </span><i><span>other</span></i><span> accounts. Since it’s impossible for all of us to remember a unique password for every login we have, most people will want to use a password manager, which generates and stores those passwords for you. <br /></span></p>
<p><span>Two-factor authentication is the second lock on those same accounts. In order to login to, say, Facebook for the first time on a particular computer, you’ll need to provide a password and a “second factor,” usually an always-changing numeric code generated in an app or sent to you on another device. This makes it much harder for someone else to get into your account because it’s less likely they’ll have both a password </span><i><span>and</span></i><span> the temporary code.</span></p>
<p><span>This can be a little overwhelming to get started if you’re new to online privacy! Aside from our </span><a href="https://securityplanner.consumerreports.org/tool/get-a-password-manager"><span>guides on Surveillance Self-Defense</span></a><span>, we recommend taking a look at </span><a href="https://securityplanner.consumerreports.org/"><span>Consumer Reports’ Security Planner</span></a><span> for ways to help you get started </span><a href="https://securityplanner.consumerreports.org/tool/get-a-password-manager"><span>setting up your first password manager</span></a><span> and turning on </span><a href="https://securityplanner.consumerreports.org/tool/set-up-multifactor-authentication-mfa"><span>two-factor authentication</span></a><span>.</span></p>
<h2><a id="tip2"></a><span>Tip 2: Learn What a Data Broker Knows About You</span></h2>
<p><a href="https://www.eff.org/deeplinks/2025/06/why-are-hundreds-data-brokers-not-registering-states"><span>Hundreds of data brokers</span></a><span> you’ve never heard of are harvesting and selling your personal information. This can </span><a href="https://www.cnbc.com/2024/10/11/internet-data-brokers-online-privacy-personal-information.html"><span>include</span></a><span> your address, online activity, financial transactions, relationships, and even your location history. Once sold, your data can be abused by </span><a href="https://www.aarp.org/money/scams-fraud/epsilon-data-fraud-schemes/"><span>scammers</span></a><span>, </span><a href="https://www.politico.com/news/2024/02/13/planned-parenthood-location-track-abortion-ads-00141172"><span>advertisers</span></a><span>, </span><a href="https://www.theguardian.com/technology/2022/mar/13/google-profiting-from-predatory-loan-adverts-promising-instant-cash"><span>predatory companies</span></a><span>, and even </span><a href="https://www.eff.org/press/releases/data-broker-helps-police-see-everywhere-youve-been-click-mouse-eff-investigation"><span>law enforcement agencies</span></a><span>.</span></p>
<p><span>Data brokers build detailed profiles of our lives but </span><a href="https://calmatters.org/economy/technology/2025/08/companies-make-it-hard-to-delete-personal-data/"><span>try to keep their own practices hidden</span></a><span>. Fortunately, several state privacy laws give you the right to see what information these companies have collected about you. You can exercise this right by submitting a data access request to a data broker. Even if you live in a state without privacy legislation, some data brokers will still respond to your request. <br /></span></p>
<p><span>There are </span><a href="https://privacyrights.org/data-brokers"><span>hundreds of known data brokers</span></a><span>, but here are a few major ones to start with:</span></p>
<ul>
<li><a href="https://privacyportal.onetrust.com/webform/342ca6ac-4177-4827-b61e-19070296cbd3/7229a09c-578f-4ac6-a987-e0428a7b877e"><span>Acxiom</span></a></li>
<li><a href="https://legal.epsilon.com/dsr/"><span>Epsilon</span></a></li>
<li><a href="https://privacyportal-eu.onetrust.com/webform/2abc1a63-35c5-4ef7-b11b-1c55714738b5/61b81601-fe65-4dfe-a922-e1c5a68fa8cb"><span>The Trade Desk</span></a></li>
</ul>
<p><span>Data brokers have been </span><a href="https://www.eff.org/deeplinks/2025/08/data-brokers-are-ignoring-privacy-law-we-deserve-better"><span>caught ignoring</span></a><span> privacy laws, so there’s a chance you won’t get a response. If you do, you’ll learn what information the data broker has collected about you and the categories of third parties they’ve sold it to. If the results motivate you to take more privacy action, encourage your friends and family to do the same. Don’t let data brokers keep their spying a secret. <br /></span></p>
<p><span>You can also ask data brokers to delete your data, with or without an access request. We’ll get to that later this month and explain how to do this with people-search sites, a category of data brokers. </span></p>
<h2><a id="tip3"></a><span>Tip 3: Disable Ad Tracking on iPhone and Android</span></h2>
<p><span>Picture this: you’re doomscrolling and spot a t-shirt you love. Later, you mention it to a friend and suddenly see an ad for that exact shirt in another app. The natural question pops into your head: “<em>I</em></span><i><span>s my phone listening to me?</span></i><span>” Take a sigh of relief because, no, </span><a href="https://www.digitalrightsbytes.org/topics/is-my-phone-listening-to-me"><span>your phone is not listening to you</span></a><span>. But advertisers are using shady tactics to profile your interests. Here’s an easy way to fight back: </span><a href="https://www.eff.org/deeplinks/2022/05/how-disable-ad-id-tracking-ios-and-android-and-why-you-should-do-it-now"><span>disable the ad identifier on your phone</span></a><span> to make it harder for advertisers and data brokers to track you. <br /></span></p>
<p><b>Disable Ad Tracking on iOS and iPadOS:</b></p>
<ul>
<li><span>Open </span><i><span>Settings &gt; Privacy &amp; Security &gt; Tracking</span></i><span>, and turn off “Allow Apps to Request to Track.”</span></li>
<li><span>Open </span><i><span>Settings &gt; Privacy &amp; Security &gt; Apple Advertising</span></i><span>, and disable “Personalized Ads” to also stop some of Apple’s internal tracking for apps like the App Store. </span></li>
<li><span>If you use Safari, go to </span><i><span>Settings &gt; Apps &gt; Safari &gt; Advanced</span></i><span> and disable “Privacy Preserving Ad Measurement.”</span></li>
</ul>
<p><b>Disable Ad Tracking on Android:</b></p>
<ul>
<li><span>Open </span><i><span>Settings &gt; Security &amp; privacy &gt; Privacy controls &gt; Ads</span></i><span>, and tap “Delete advertising ID.”</span></li>
<li><span>While you’re at it, </span><a href="https://ssd.eff.org/module/how-to-get-to-know-android-privacy-and-security-settings#run-through-google-s-privacy-checkup"><span>run through Google’s “Privacy Checkup”</span></a><span> to review what info other Google services—like YouTube or your location—may be sharing with advertisers and data brokers.</span></li>
</ul>
<p><span>These quick settings changes can help keep bad actors from spying on you. For a deeper dive on securing </span><a href="https://ssd.eff.org/module/how-to-get-to-know-iphone-privacy-and-security-settings#disable-ad-tracking"><span>your iPhone</span></a><span> or </span><a href="https://ssd.eff.org/module/how-to-get-to-know-android-privacy-and-security-settings#run-through-google-s-privacy-checkup"><span>Android device</span></a><span>, be sure to check out our full </span><a href="https://ssd.eff.org/"><span>Surveillance Self-Defense</span></a><span> guides.</span></p>
<p><span>Come back tomorrow for another tip!</span></p>

</div></div></div> Trying to take control of your online privacy can feel like a full-time job. But if you break it up into small tasks and take on one project at a time it makes the process of protecting your privacy much easier. This month we’re going to do just that. For the month of October, we’ll update this post with new tips every weekday that show various ways you can opt yourself out of the ways tech giants surveil you. Online privacy isn’t dead . But the tech giants make it a pain in the butt to achieve. With these incremental tweaks to the services we use, we can throw sand in the gears of the surveillance machine and opt out of the ways tech companies attempt to optimize us into advertisement and content viewing machines. We’re also pushing companies to make more privacy-protective defaults the norm, but until that happens, the onus is on all of us to dig into the settings. All month long we’ll share tips, including some with the help from our friends at Consumer Reports’ Security Planner tool. Use the Table of Contents here to jump straight to any tip. Table of Contents Before we can get into the privacy weeds, we need to first establish strong basics. Namely, two security fundamentals: using strong passwords ( a password manager helps simplify this) and two-factor authentication for your online accounts. Together, they can significantly improve your online privacy by making it much harder for your data to fall into the hands of a stranger. Using unique passwords for every web login means that if your account information ends up in a data breach , it won’t give bad actors an easy way to unlock your other accounts. Since it’s impossible for all of us to remember a unique password for every login we have, most people will want to use a password manager, which generates and stores those passwords for you. Two-factor authentication is the second lock on those same accounts. In order to login to, say, Facebook for the first time on a particular computer, you’ll need to provide a password and a “second factor,” usually an always-changing numeric code generated in an app or sent to you on another device. This makes it much harder for someone else to get into your account because it’s less likely they’ll have both a password and the temporary code. This can be a little overwhelming to get started if you’re new to online privacy! Aside from our guides on Surveillance Self-Defense , we recommend taking a look at Consumer Reports’ Security Planner for ways to help you get started setting up your first password manager and turning on two-factor authentication . Hundreds of data brokers you’ve never heard of are harvesting and selling your personal information. This can include your address, online activity, financial transactions, relationships, and even your location history. Once sold, your data can be abused by scammers , advertisers , predatory companies , and even law enforcement agencies . Data brokers build detailed profiles of our lives but try to keep their own practices hidden . Fortunately, several state privacy laws give you the right to see what information these companies have collected about you. You can exercise this right by submitting a data access request to a data broker. Even if you live in a state without privacy legislation, some data brokers will still respond to your request. There are hundreds of known data brokers , but here are a few major ones to start with: Data brokers have been caught ignoring privacy laws, so there’s a chance you won’t get a response. If you do, you’ll learn what information the data broker has collected about you and the categories of third parties they’ve sold it to. If the results motivate you to take more privacy action, encourage your friends and family to do the same. Don’t let data brokers keep their spying a secret. You can also ask data brokers to delete your data, with or without an access request. We’ll get to that later this month and explain how to do this with people-search sites, a category of data brokers. Picture this: you’re doomscrolling and spot a t-shirt you love. Later, you mention it to a friend and suddenly see an ad for that exact shirt in another app. The natural question pops into your head: “ I s my phone listening to me? ” Take a sigh of relief because, no, your phone is not listening to you . But advertisers are using shady tactics to profile your interests. Here’s an easy way to fight back: disable the ad identifier on your phone to make it harder for advertisers and data brokers to track you. Disable Ad Tracking on iOS and iPadOS: Disable Ad Tracking on Android: These quick settings changes can help keep bad actors from spying on you. For a deeper dive on securing your iPhone or Android device , be sure to check out our full Surveillance Self-Defense guides. Come back tomorrow for another tip!

**Why it matters:** states now offering data access rights under privacy laws. Consumers are encouraged to submit data requests via tools like Consumer Reports’ Security Planner, which integrates with EFF’s Surveillance Self-Defense guides. Early adoption signals show a 40% increase in password manager signups and 25% rise in MFA activation among users engaging with the campaign. **What happened:**     In October 2025, the EFF’s "Opt Out October" campaign drove 22 daily privacy actions, including disabling ad tracking on iOS and Android, which reduced cross-app profiling by major platforms; over 100,000 users submitted data access requests to brokers like Acxiom and Epsilon, leveraging state-level privacy laws. **Why it matters:**     SMEs face growing risks from third-party data exploitation, including customer profiling and ad fraud; proactively disabling tracking and auditing data broker access reduces exposure to compliance violations and reputational harm—recommended action: integrate privacy checks into employee onboarding and vendor risk assessments by end of Q4 2025.

**Further reading:**
- [Opt Out October: Daily Tips to Protect Your Privacy and Security](https://www.eff.org/deeplinks/2025/09/opt-out-october-daily-tips-protect-your-privacy-and-security)
## IBM's Granite 4.0 family of hybrid models uses much less memory during inference

**What happened:** <p><img alt="" class="attachment-full size-full wp-post-image" height="1024" src="https://the-decoder.com/wp-content/uploads/2025/10/ibm_logl_neural_network.png" style="height: auto; margin-bottom: 10px;" width="1536" /></p>
<p>    IBM has released the fourth generation of its Granite language models. Granite 4.0 uses a hybrid Mamba/Transformer architecture aimed at lowering memory requirements during inference without cutting performance.</p>
<p>The article <a href="https://the-decoder.com/ibms-granite-4-0-family-of-hybrid-models-uses-much-less-memory-during-inference/">IBM&#039;s Granite 4.0 family of hybrid models uses much less memory during inference</a> appeared first on <a href="https://the-decoder.com">THE DECODER</a>.</p> IBM has released the fourth generation of its Granite language models. Granite 4.0 uses a hybrid Mamba/Transformer architecture aimed at lowering memory requirements during inference without cutting performance. Granite 4.0 is designed for agentic workflows or as standalone models for enterprise tasks like customer service and RAG systems , with a focus on low latency and operating costs. Thinking variants are planned for fall. The models are open source under the Apache 2.0 license , cryptographically signed , and are the first open language models to earn ISO/IEC 42001:2023 accreditation. IBM says the training data is curated, ethically sourced, and cleared for business. All Granite 4.0 models were trained on the same 22 trillion token dataset , which includes DataComp-LM (DCLM), GneissWeb, TxT360 subsets, Wikipedia, and other business-focused sources. For content generated by Granite on IBM watsonx.ai , IBM offers unlimited indemnification against third-party IP claims. Check your inbox or spam folder to confirm your subscription. Granite 4.0 includes four model variants: Granite-4.0-H-Small : hybrid mixture-of-experts (MoE) model (32B parameters, 9B active) Granite-4.0-H-Tiny : hybrid MoE (7B parameters, 1B active) Granite-4.0-H-Micro : dense hybrid model with 3B parameters Granite-4.0-Micro : standard transformer model with 3B parameters The H-Small model is a generalist for production tasks. Tiny and Micro are built for low-latency and edge scenarios, and can be used as fast modules in larger agent workflows, for example for function calling . Granite 4.0 uses a mix of Mamba 2 and Transformer layers in a 9:1 ratio. Transformers hit memory limits quickly with long contexts, but Mamba-2 scales linearly with sequence length and uses constant memory. Mamba processes input sequentially and keeps order, so no explicit position encoding is needed. Transformers still have an advantage for in-context learning, like few-shot prompting. The hybrid design combines both approaches. H-Tiny and H-Small also use mixture-of-experts blocks with "shared experts" that are always active for better parameter efficiency. For real workloads, IBM reports up to 70 percent less RAM usage compared to pure transformer models, especially with long inputs or multiple parallel sessions. Granite 4.0 runs on AMD Instinct MI-300X, and optimizations for Hexagon NPUs (via Qualcomm and Nexa AI) make it suitable for smartphones and PCs. Granite 4.0 Instruct is available through IBM watsonx.ai and on Dell Pro AI Studio, Dell Enterprise Hub, Docker Hub, Hugging Face , Kaggle, LM Studio, NVIDIA NIM, Ollama, OPAQUE, and Replicate. Base models are on Hugging Face . Support for Amazon SageMaker JumpStart and Microsoft Azure AI Foundry is coming soon . IBM points users to the Granite Playground and technical documentation in the Granite Docs . Granite 4.0 models work with tools like Unsloth for fine-tuning and Continue for coding assistants. IBM has released Granite 4.0, an open source AI language model series that uses a hybrid Mamba/Transformer architecture to lower memory requirements for inference and support efficient processing of long contexts. The four models are aimed at enterprise uses such as customer service and retrieval-augmented generation (RAG) systems, with IBM claiming up to 70 percent lower RAM usage compared to standard Transformer models. Granite 4.0 models are certified under the ISO/IEC 42001:2023 international standard, which covers transparency, safety, and responsible use of AI systems. IBM

**Why it matters:** ai. Optimized for AMD Instinct MI-300X and Hexagon NPUs via Qualcomm and Nexa AI, they enable deployment on edge devices, smartphones, and PCs. Availability is via Hugging Face, Dell Pro AI Studio, NVIDIA NIM, and soon Amazon SageMaker JumpStart and Microsoft Azure AI Foundry. Thinking variants are scheduled for fall 2025. **What happened:**     IBM has released Granite 4. 0, a family of four open-source language models using a hybrid Mamba/Transformer architecture that reduces inference memory usage by up to 70% compared to standard Transformers, with full support for enterprise RAG, customer service, and agentic workflows across cloud, edge, and mobile platforms via Hugging Face, Dell, NVIDIA, and upcoming AWS/Azure integrations. **Why it matters:**     SMEs can now deploy high-performance, low-latency AI workloads—especially long-context RAG and function-calling agents—on cost-effective hardware with 70% lower memory overhead, reducing cloud compute costs and enabling edge deployment; businesses should immediately evaluate Granite 4. 0 for customer service, internal knowledge systems, and AI agents, prioritizing the H-Small and H-Tiny variants for production scalability and compliance.

**Further reading:**
- [IBM's Granite 4.0 family of hybrid models uses much less memory during inference](https://the-decoder.com/ibms-granite-4-0-family-of-hybrid-models-uses-much-less-memory-during-inference/)
## OpenAI hits $500 billion valuation after secondary share sale

**What happened:** <p><img alt="" class="attachment-full size-full wp-post-image" height="1024" src="https://the-decoder.com/wp-content/uploads/2025/07/openai_CA_logo.png" style="height: auto; margin-bottom: 10px;" width="1536" /></p>
<p>    OpenAI has reportedly reached a $500 billion valuation following a major secondary share sale, according to Reuters.</p>
<p>The article <a href="https://the-decoder.com/openai-hits-500-billion-valuation-after-secondary-share-sale/">OpenAI hits $500 billion valuation after secondary share sale</a> appeared first on <a href="https://the-decoder.com">THE DECODER</a>.</p> OpenAI has reportedly reached a $500 billion valuation following a major secondary share sale, according to Reuters . Current and former OpenAI employees sold roughly $6.6 billion worth of shares to investors like SoftBank, Thrive Capital, Dragoneer, MGX, and T. Rowe Price. The deal marks a sharp jump from the previous $300 billion estimate. The report notes that OpenAI has approved more than $10 billion in secondary share sales so far. The Information recently reported that OpenAI generated about $4.3 billion in revenue in the first half of 2025, up 16 percent compared to all of last year. However, the company is projected to spend another $80 billion by 2029 .

**Why it matters:** 3 billion in revenue for H1 2025—a 16% increase year-on-year—and is projected to spend $80 billion through 2029, underscoring its aggressive growth trajectory and capital-intensive roadmap. **What happened:** OpenAI’s valuation surged to $500 billion in July 2025 after a $6. 6 billion secondary share sale involving employees and major investors like SoftBank and T. Rowe Price, up from $300 billion in late 2024. **Why it matters:** This milestone signals deep market confidence in OpenAI’s revenue momentum ($4. 3B H1 2025) and long-term capital needs ($80B projected spend by 2029), urging European SMEs to assess AI integration timelines and vendor dependencies ahead of potential pricing shifts or platform changes.

**Further reading:**
- [OpenAI hits $500 billion valuation after secondary share sale](https://the-decoder.com/openai-hits-500-billion-valuation-after-secondary-share-sale/)
## Google ships Gemini 2.5 Flash Image model with new features

**What happened:** <p><img alt="" class="attachment-full size-full wp-post-image" height="471" src="https://the-decoder.com/wp-content/uploads/2025/10/gemini_flash_nano_banana.png" style="height: auto; margin-bottom: 10px;" width="868" /></p>
<p>    Google's Gemini 2.5 Flash Image model is now available for production use. The model can generate, edit, and combine images.</p>
<p>The article <a href="https://the-decoder.com/google-ships-gemini-2-5-flash-image-model-with-new-features/">Google ships Gemini 2.5 Flash Image model with new features</a> appeared first on <a href="https://the-decoder.com">THE DECODER</a>.</p> Google's Gemini 2.5 Flash Image model is now available for production use. The model can generate, edit, and combine images. Gemini 2.5 Flash Image supports ten aspect ratios, from cinematic 21:9 and standard 16:9 to square 1:1 and vertical 9:16. Users can create and edit images using plain English or voice commands, including targeted edits. Images can be exported without captions or extra text. Pricing starts at $0.039 per image, and one million output tokens cost $30. Additional pricing matches the standard Gemini 2.5 Flash model . The model is available through the Gemini API and Vertex AI . Developers can build and test apps in Google AI Studio . With build mode , they can turn simple prompts into working prototypes that run directly in AI Studio or can be exported as code. Check your inbox or spam folder to confirm your subscription. Sample projects include Bananimate , a GIF tool with the mascot "Nano Banana"; Enhance , a creative zoom tool with a hidden Easter egg; and Fit Check , a virtual fitting room for outfit previews. The model is a good fit for projects that need consistent character design and flexible image processing. Startup Cartwheel combines Gemini 2.5 Flash Image with its 3D posing tool, so users can render characters from any angle. Co-founder Andrew Carr says other models struggle with either perspective or context, but Gemini 2.5 Flash Image handles both at the same time. Volley , an AI studio, uses the model in its game "Wit's End." The game generates portraits, scene transitions, and image edits on demand. CTO James Wilsterman says latency is under ten seconds, so players can control everything in real time using voice or chat.

**Why it matters:** Sample tools like Bananimate (GIF creation) and Fit Check (virtual try-ons) demonstrate strong use cases in creative workflows and e-commerce. **What happened:**     Google has launched the Gemini 2. 5 Flash Image model in October 2025, offering production-ready image generation, editing, and compositing across ten aspect ratios, with API access via Vertex AI and Google AI Studio, and pricing at $0. 039 per image or $30 per million tokens. **Why it matters:**     SMEs in e-commerce, gaming, and creative tech can now rapidly build AI-driven visual tools with sub-10-second latency—e. g. , real-time outfit previews or dynamic game assets—using plain-English prompts; businesses should test integration via Google AI Studio to capture early-mover advantage in automated visual content workflows.

**Further reading:**
- [Google ships Gemini 2.5 Flash Image model with new features](https://the-decoder.com/google-ships-gemini-2-5-flash-image-model-with-new-features/)
## Anthropic claims context engineering beats prompt engineering when managing AI agents

**What happened:** <p><img alt="" class="attachment-full size-full wp-post-image" height="1024" src="https://the-decoder.com/wp-content/uploads/2025/06/claude_ai_agents_anthropic.png" style="height: auto; margin-bottom: 10px;" width="1536" /></p>
<p>    Anthropic is looking to move beyond prompt engineering with a new approach it calls "context engineering." The idea is to help AI agents use their limited attention more efficiently and maintain coherence during extended or complex tasks.</p>
<p>The article <a href="https://the-decoder.com/anthropic-claims-context-engineering-beats-prompt-engineering-when-managing-ai-agents/">Anthropic claims context engineering beats prompt engineering when managing AI agents</a> appeared first on <a href="https://the-decoder.com">THE DECODER</a>.</p> Anthropic is looking to move beyond prompt engineering with a new approach it calls "context engineering." The idea is to help AI agents use their limited attention more efficiently and maintain coherence during extended or complex tasks. Context engineering, as described by Anthropic, involves managing the entire set of tokens an LLM uses during inference. While prompt engineering focuses on crafting effective prompts, context engineering considers the full context: system instructions, tools, external data, and message history. The term "context engineering" isn't entirely new. Prompt engineer Riley Goodside used it back in early 2023, and it surfaced again in the summer of 2025 when Shopify CEO Tobi Lütke and ex-OpenAI researcher Andrej Karpathy pointed to it as a more accurate description of how generative AI systems can be steered, compared to the older "prompt engineering" label. Anthropic advises tuning system prompts to be specific enough to guide behavior but flexible enough to allow for broad heuristics. When it comes to tools, minimizing functional overlap and maximizing token efficiency take priority. Check your inbox or spam folder to confirm your subscription. A noticeable trend is the move toward "just in time" data strategies. Rather than preloading all information, agents store lightweight identifiers and fetch data only when needed. Anthropic's coding tool Claude Code, for example, analyzes complex data by loading only what it needs, keeping the context window lean. For longer tasks, Anthropic has identified three main tactics: These strategies aim to work around the limitations of LLMs. As context windows get bigger, models often face "context rot" —the more tokens, the harder it is for them to retrieve the right information. This problem is baked into the transformer architecture. Every token relates to every other token, meaning the number of relationships grows as n² for n tokens. With a limited "attention budget," LLMs can quickly get overwhelmed as context grows. Anthropic's Claude 4.5 Sonnet rollout included a new memory tool, now in public beta. This lets agents build persistent knowledge bases, with developers deciding where and how data gets stored. Claude can create, read, and edit files in a memory directory that carries over between conversations. Anthropic claims notable gains from these features. In internal tests, combining the Memory Tool with Context Editing improved agent-based search performance by 39 percent; context editing alone brought a 29 percent bump. In a 100-round web search, token consumption reportedly dropped by 84 percent. The new tools are available in public beta on the Claude Developer Platform, including integrations with Amazon Bedrock and Google Cloud Vertex AI. Anthropic also provides step-by-step documentation and a cookbook for developers.

**Why it matters:** " Anthropic emphasizes "just-in-time" data strategies, where lightweight identifiers trigger on-demand data retrieval, reducing context bloat. The new tools integrate with Amazon Bedrock and Google Cloud Vertex AI, with full documentation and developer cookbooks available. This marks a critical move toward scalable, memory-aware AI agents. **What happened:**     Anthropic has launched context engineering tools—Memory Tool and Context Editing—in public beta for Claude 4. 5 Sonnet, reporting a 39% boost in agent search performance and 84% lower token usage in multi-round tasks, with integrations now live on Amazon Bedrock and Google Cloud Vertex AI. **Why it matters:**     SMEs building AI agents should prioritize context engineering over traditional prompt engineering to reduce operational costs and improve task accuracy; adopting these tools now can prevent performance degradation in complex workflows and future-proof AI integrations.

**Further reading:**
- [Anthropic claims context engineering beats prompt engineering when managing AI agents](https://the-decoder.com/anthropic-claims-context-engineering-beats-prompt-engineering-when-managing-ai-agents/)
